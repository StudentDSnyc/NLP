{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Processing (NLP) Vs Machine Learning (ML)\n",
    "- NLP broadly refers to the study and development of computer systems that can interpret speech and text as humans naturally speak and write (i.e. NLP is the study of making computers understand how humans naturally speak, write and communicate)\n",
    "- Human communication has inconsistencies - misspellings, abbreviations, slangs, idioms. These inconsistencies make things difficult for computers to understand how humans naturally speak, write and communicate\n",
    "- Modern techniques and approaches for NLP are based on **machine learning**\n",
    "- **Machine Learning** (in the context of text analytics) is a set of statistical techniques for identifying some aspect of text (part-of-speech, entities, sentiment, etc.) \n",
    "    - Supervised: Take a bunch of documents that have been “tagged” for some feature, like part-of-speech, entities, or topics (classifiers). Use these documents as a training set that produces a statistical model. Apply this model to new text to predict \"tags\"\n",
    "        - *Techniques*: Support Vector Machines, Naive Bayes, Decision Trees, Random Forest, Neural Networks, Maximum Entropy, Bayesian Networks, Conditional Random Field, etc.\n",
    "    - Unsupervised: Take a large set of documents and use statistical techniques to extract meaning.\n",
    "        - *Techniques*: Clustering, Latent Semantic Indexing, Matrix Factorization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag-of-words model**\n",
    "- A simplifying document representation used in NLP and information retrieval. \n",
    "- Grammar and word order is disregarded.\n",
    "- Represents a document by the occurrence counts of each word\n",
    "\n",
    "Example:\n",
    "\n",
    "- $d_1$ = Jon uses swords. Jon likes Ygritte \n",
    "- $d_2$ = Oberyn uses spears and likes dagger\n",
    "- $d_3$ = Ygritte uses bow and arrow\n",
    "    \n",
    ">*Process the documents (lowercase strings, remove stopwords, etc.) and Create a vocabulary from documents*\n",
    "\n",
    "- Vocabulary = {jon, likes, swords, oberyn, uses, spears, and, dagger, ygritte, bow, arrow}  \n",
    "\n",
    ">*To get Bag-of-words: Count the number of times each word occurs in each document*\n",
    "\n",
    "![bow](images/bow.png)\n",
    "\n",
    "**Term Frequency** $tf_{t,d}$ of term $t$ in document $d$ is defined as the number of times $t$ occurs in $d$\n",
    "- Term Frequency measures how frequently a term occurs in a document\n",
    "- Normalized Term Frequency - Term Frequency count is normalized to prevent a bias towards longer document.\n",
    " $$tf_{t,d}=\\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}$$\n",
    "\n",
    "**Document Frequency**\n",
    "- Measures commonness, e.g. \"the\" is most common and \"imp\" is least common\n",
    "![df](images/df.png)\n",
    "\n",
    "**Inverse Document Frequency** - is a measure of how much information the word provides, i.e. whether the term is common or rare across all documents\n",
    "- Estimates the *rarity* of a term in the whole document collection\n",
    "- If a term occurs in all the documents of the collection, its IDF = 0\n",
    "- Inverse Document Frequency is given by:\n",
    "$$idf_t = log\\frac{N}{df_t}$$\n",
    "where: $N$ is the total number of documents in a collection. Log is used to dampen the effect of idf. *To avoid division-by-zero when the term is not in the corpus, 1 is added to $df_t$ in the denominator *\n",
    "![idf](images/idf.png)\n",
    "- idf of a rare term is likely to be high and idf of a frequent term is likely to be low\n",
    "\n",
    "**tf-idf Weighting**\n",
    "- tf-idf weight of a term $t$ is the product of its $tf$ weight and its $idf$ weight: tf-$idf_{t,d} = tf_{t,d}.idf_t$\n",
    "- tf-idf increases with the rarity of the term in the collection\n",
    "- High weight tf-idf is reached by high term frequency in a given document and a low document frequency of the term in the whole collection of documents\n",
    "- tf-$idf_{t,d}$ assigns to term $t$ a weight in document $d$ that is:\n",
    "    - Highest when $t$ occurs many times within a small number of documents\n",
    "    - Lower when the term occurs fewer times in a document, or occurs in many documents\n",
    "    - Lowest when the term occurs in virtually all documents\n",
    "\n",
    "---\n",
    "\n",
    "[Stanford IR Book](http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html)  \n",
    "\n",
    "[TFIDF Blogpost](http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "Set of techniques that analyze the words in documents to discover the themes that run through them and annotate large archives of documents with thematic information. Often, the number of topics to be discovered is predefined. Topic modeling is a way of extrapolating backward from a collection of documents to infer the “topics” that could have generated them. \n",
    "\n",
    "- A topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic model types:\n",
    "    - Linear Algebra based\n",
    "    - Probabilistic Modeling based\n",
    "\n",
    "**Uses**\n",
    "- Exploring large text corpora (finding patterns)\n",
    "- Information retrieval: Finding documents matching a query\n",
    "- Clustering and similarity\n",
    "- Recommender systems (Collaborative filtering)\n",
    "- Pre-processing step before applying supervised learning methods\n",
    "\n",
    "### Models\n",
    "**TF-IDF**\n",
    "- Product of TF (term frequency in a document) and IDF (inverse document frequency of term in corpus)\n",
    "- Intuition: Give high weight to words that are topic-specific\n",
    "\n",
    "**Latent Semantic Analysis/Indexing (LSA/LSI)**\n",
    "![lsi](images/lsi.png)\n",
    "- Singular Value Decomposition (SVD) of the frequency/tf-idf matrix, followed by dimensionality reduction\n",
    "- Intuition: Denoise to extract latent factors/topics\n",
    "\n",
    "**LDA and HDP**\n",
    "- Latent Drichlet Allocation (LDA) - Probabilistic extension of Latent Semantic Analysis, i.e. pLSA. Generative probabilistic model, that assumes a Dirichlet prior over the latent topics\n",
    "- Hierarchical Dirichlet Process (HDP) - Nonparametric generalization of LDA (No more setting a fixed number of topics)\n",
    "- Intuition: Bayesian statistics\n",
    "\n",
    "**Word2Vec and Doc2Vec**\n",
    "- Word2Vec - Representing words as vectors (inspired by deep learning)\n",
    "\n",
    "\n",
    "--- \n",
    "Resources\n",
    "- https://www.lucypark.kr/courses/2015-ba/text-mining.html#2-tokenize\n",
    "- http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/\n",
    "- https://dato.com/learn/gallery/notebooks/sherlock_text_analytics.html\n",
    "- http://www.yseam.com/blog/WV.html\n",
    "- NNFM (Scikit-Learn): http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "- http://derekgreene.com/notebooks/nmf_topic_modeling.py\n",
    "- http://derekgreene.com/nmf-topic/\n",
    "- http://tech.opentable.com/2015/01/12/finding-key-themes-from-free-text-reviews/\n",
    "- https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/\n",
    "- TFIDF LDA: http://gensim.narkive.com/QvcqFjyj/gensim-2588-poor-similarity-results-for-lda-and-hdp-vs-lsi-and-tf-idf\n",
    "- https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "LDA is a generative statistical model (a model for randomly generating observable data values given some hidden parameters) that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n",
    "\n",
    "**LDA Assumption**\n",
    "- Topic distribution is assumed to have a [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) prior\n",
    "- Documents are generated randomly\n",
    "\n",
    "![topic](images/topic.png)\n",
    "- Every topic is a distribution (weighted list) over words: Topic is a distribution over a fixed vocabulary, **each topic contains each word from the dictionary (but some words have very low probability)**\n",
    "- Each word in the document is randomly selected from randomly selected topic from distribution of topics\n",
    "- Every document has a distribution over topics: Each document *exhibit multiple topics in different proportions*, i.e. **All the documents in the collection share the same set of topics**, but each document exhibits those topics in different proportions\n",
    "\n",
    "- Graph Model for LDA\n",
    "![graphicalModel](images/lda.png)\n",
    "\n",
    "- LDA is sensitive to corpus preprocessing and training parameters (stopword removal, number of training passes, number of topic produced)\n",
    "- LDA is useful when human interpretation/visualization of topics is needed\n",
    "- Probabilistic methods are less robust than vector-space methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea behind LDA\n",
    "\n",
    "**Generating (writing) an article on \"Japanese Cuisine\"**\n",
    "\n",
    "Three elements are needed:\n",
    "- Words\n",
    "- Topics\n",
    "- Documents\n",
    "---\n",
    "\n",
    "1. What topics?\n",
    "    - Heavy draw from topics about: Fish, Rice, Miso, Ramen, etc.\n",
    "    - Weak draw from topics about: Milk, Cheese, Mexico, NASA, etc.\n",
    "2. Assign weight to topics\n",
    "![wts](images/wts.png)\n",
    "3. Pull words from topic bag-of-words and start writing (Each topic is a bag filled with words) until article is complete.\n",
    "    - Pulling words from topics bag-of-words according to topic weight to start writing, e.g. Reaching into Fish bag-of-words for 7% of words in the Japanese Cuisine article\n",
    "   \n",
    "> Result: Document about Japanese Cuisine\n",
    "\n",
    "**Importance of Generative Model** - Ability to work backwards. \n",
    "- If a non sensical document can be generated using generative model then it is possible to reverse the process and infer, given any new document and a topic model (already generated), what the topics are that the new document draws from.\n",
    "\n",
    "---\n",
    "\n",
    "> Need to identify underlying topics that organize collection of documents\n",
    "\n",
    "**Assumption**\n",
    "- Each document contains a mixture of topics\n",
    "- A topic can be understood as a collection of words that have different probabilities of appearance in passages discussing topics\n",
    "    - One topic may contain many occurances of \"salmon\", \"tuna\", \"cod\" and few occurances of \"milk\"\n",
    "    - Another topic may contain many occurances of \"milk\", \"cheese\", \"cow\" and few occurances of \"tuna\"\n",
    "    \n",
    "Suppose which topic produced every word in the collection is known except for one word $w$ in the document $d$. How to decide whether the occurance of $w$ belongs to topic $t$? Consider:\n",
    " 1. How often the word $w$ appear in topic $t$\n",
    "     - If \"NASA\" often occurs in topic \"Fish\" then \"NASA\" might belong to topic \"Fish\". A word can be common in more that one topic so it is not a good idea to assign \"NASA\" to topic about \"Fish\" if the document is about \"Space\"\n",
    " 2. How common is topic $t$ in the document \n",
    "    \n",
    "    \n",
    "$$P(t|w,d) = \\frac{\\text{# of word w in topic t} + \\beta_w}{\\text{total tokens in t}+\\beta}*(\\text{# of words in d that belong to t}+\\alpha)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**\n",
    "\n",
    "![vectors](images/vectors.gif)\n",
    "\n",
    "- [Pre-trained Word and Vector Data from Google](https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors)\n",
    "- [Doc2Vec Notebook](http://nbviewer.jupyter.org/github/fbkarsdorp/doc2vec/blob/master/doc2vec.ipynb)\n",
    "- [Blog on Word2Vec by StitchFix (Good References)](http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/)\n",
    "- [Word2Vec/Doc2Vec using Chinese Restaurant Process](http://eng.kifi.com/from-word2vec-to-doc2vec-an-approach-driven-by-chinese-restaurant-process/)\n",
    "- [GloVe Python](https://github.com/maciejkula/glove-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import NLP Libraries\n",
    "from gensim import corpora, models\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (unigram)\n",
    "\n",
    "**Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample documents (Quotes from Game of Thrones T.V. Series)\n",
    "doc_0 = \"It is rare to meet a Lannister who share my enthusiasm for dead Lannisters.\"\n",
    "doc_1 = \"Arya Stark hasn't been seen since her father was killed. Where do you think she is? My money's on dead.\\\n",
    "         There's a certain safety in death, wouldn't you say?\"\n",
    "doc_2 = \"It's not easy being drunk all the time. If it were easy, everyone would do it.\"\n",
    "doc_3 = \"He would see this country burn if he could be king of the ashes.\"\n",
    "doc_4 = \"I will hurt you for this. A day will come when you think you are safe and happy, and your joy will turn to \\\n",
    "         ashes in your mouth. And you will know the debt is paid.\"\n",
    "doc_5 = \"All my life men like you have sneered at me. And all my life I've been knocking men like you into the dust.\"\n",
    "doc_6 = \"Power resides where men believe it resides. It's a trick, a shadow on the wall. And a very small man can \\\n",
    "         cast a very large shadow.\"\n",
    "doc_7 = \"It's the family name that lives on. It's all that lives on. Not your personal glory, not your honor...but family.\"\n",
    "doc_8 = \"I am a Khaleesi of the Dothraki. I am the wife of the great Khal and I carry his son inside me. The next time \\\n",
    "         you raise a hand to me will be the last time you have hands.\"\n",
    "doc_9 = \"Chaos isn't a pit. Chaos is a ladder. Many who try to climb it fail and never get to try again. \\\n",
    "         The fall breaks them. And some are given a chance to climb, they cling to the realm or the gods or love. \\\n",
    "         only the ladder is real. The climb is all there is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of sample documents\n",
    "doc_list = [doc_0, doc_1, doc_2, doc_3, doc_4, doc_5, doc_6, doc_7, doc_8, doc_9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning Documents**\n",
    "- Lower case\n",
    "- Tokenization\n",
    "- Removing stop words\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arya stark hasn't been seen since her father was killed. where do you think she is? my money's on dead.         there's a certain safety in death, wouldn't you say?\n"
     ]
    }
   ],
   "source": [
    "# Lower Case\n",
    "print doc_1.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arya', 'stark', 'hasn', 't', 'been', 'seen', 'since', 'her', 'father', 'was', 'killed', 'where', 'do', 'you', 'think', 'she', 'is', 'my', 'money', 's', 'on', 'dead', 'there', 's', 'a', 'certain', 'safety', 'in', 'death', 'wouldn', 't', 'you', 'say']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization - NLTK\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "print tokenizer.tokenize(doc_1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arya', 'stark', 'seen', 'since', 'father', 'killed', 'think', 'money', 'dead', 'certain', 'safety', 'death', 'say']\n"
     ]
    }
   ],
   "source": [
    "# Removing \"stop words\"\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "print [t for t in tokenizer.tokenize(doc_1.lower()) if t not in en_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'arya', u'stark', u'seen', u'sinc', u'father', u'kill', u'think', u'money', u'dead', u'certain', u'safeti', u'death', u'say']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create a stemmer of class PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem the tokens\n",
    "print [stemmer.stem(t) for t in [t for t in tokenizer.tokenize(doc_1.lower()) if t not in en_stopwords]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'rare', u'meet', u'lannist', u'share', u'enthusiasm', u'dead', u'lannist'], [u'arya', u'stark', u'seen', u'sinc', u'father', u'kill', u'think', u'money', u'dead', u'certain', u'safeti', u'death', u'say'], [u'easi', u'drunk', u'time', u'easi', u'everyon', u'would'], [u'would', u'see', u'countri', u'burn', u'could', u'king', u'ash'], [u'hurt', u'day', u'come', u'think', u'safe', u'happi', u'joy', u'turn', u'ash', u'mouth', u'know', u'debt', u'paid'], [u'life', u'men', u'like', u'sneer', u'life', u'knock', u'men', u'like', u'dust'], [u'power', u'resid', u'men', u'believ', u'resid', u'trick', u'shadow', u'wall', u'small', u'man', u'cast', u'larg', u'shadow'], [u'famili', u'name', u'live', u'live', u'person', u'glori', u'honor', u'famili'], [u'khaleesi', u'dothraki', u'wife', u'great', u'khal', u'carri', u'son', u'insid', u'next', u'time', u'rais', u'hand', u'last', u'time', u'hand'], [u'chao', u'pit', u'chao', u'ladder', u'mani', u'tri', u'climb', u'fail', u'never', u'get', u'tri', u'fall', u'break', u'given', u'chanc', u'climb', u'cling', u'realm', u'god', u'love', u'ladder', u'real', u'climb']]\n"
     ]
    }
   ],
   "source": [
    "# Processing the documents\n",
    "\n",
    "texts = [] # List for storing cleaned document tokens\n",
    "\n",
    "for doc in doc_list:\n",
    "    # Lower case\n",
    "    raw_doc = doc.lower()\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(raw_doc)\n",
    "    # Remove stop words\n",
    "    tokens_remaining = [t for t in tokens if t not in en_stopwords]\n",
    "    # Stemmed tokens\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in tokens_remaining]\n",
    "    # Add tokens to the list\n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "print texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Document-Term Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(92 unique tokens: [u'money', u'kill', u'death', u'easi', u'father']...)\n",
      "{u'money': 8, u'kill': 13, u'death': 6, u'easi': 19, u'father': 9, u'carri': 67, u'enthusiasm': 3, u'real': 75, u'get': 78, u'safe': 30, u'break': 83, u'know': 36, u'fall': 85, u'shadow': 54, u'day': 39, u'like': 42, u'tri': 74, u'knock': 40, u'resid': 46, u'name': 57, u'dothraki': 68, u'small': 53, u'arya': 16, u'dead': 2, u'see': 26, u'fail': 86, u'safeti': 7, u'power': 47, u'god': 79, u'larg': 51, u'burn': 25, u'khaleesi': 63, u'insid': 64, u'joy': 29, u'men': 43, u'believ': 52, u'sneer': 44, u'debt': 37, u'come': 38, u'sinc': 11, u'great': 62, u'last': 72, u'would': 21, u'could': 24, u'turn': 33, u'countri': 27, u'climb': 88, u'honor': 61, u'chanc': 91, u'love': 87, u'drunk': 18, u'wall': 48, u'son': 69, u'famili': 56, u'lannist': 4, u'happi': 32, u'given': 76, u'ash': 28, u'khal': 65, u'stark': 10, u'next': 70, u'live': 59, u'life': 41, u'hurt': 34, u'rare': 0, u'king': 23, u'trick': 49, u'cast': 50, u'glori': 60, u'meet': 5, u'everyon': 20, u'certain': 14, u'share': 1, u'chao': 82, u'say': 12, u'seen': 15, u'pit': 90, u'realm': 77, u'cling': 84, u'ladder': 80, u'paid': 31, u'hand': 73, u'rais': 71, u'mouth': 35, u'never': 81, u'mani': 89, u'dust': 45, u'man': 55, u'wife': 66, u'think': 17, u'person': 58, u'time': 22}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary using a list of lists\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print dictionary\n",
    "print dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Convert dictionary to bag-of-words\n",
    "bow = [dictionary.doc2bow(text) for text in texts]\n",
    "print bow[0] # Tuples are (term ID, term frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply LDA Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = models.ldamodel.LdaModel(bow, num_topics = 5, id2word = dictionary, passes = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.011*resid + 0.011*shadow + 0.011*man'), (1, u'0.090*easi + 0.049*would + 0.049*drunk'), (2, u'0.058*men + 0.040*life + 0.040*like'), (3, u'0.047*time + 0.047*hand + 0.026*ash'), (4, u'0.052*climb + 0.036*tri + 0.036*ladder')]\n"
     ]
    }
   ],
   "source": [
    "print lda_model.print_topics(num_topics = 5, num_words = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable word in topic \"Topic Id\" with Probability \n",
      "\n",
      "Topic # 0\n",
      "Word: resid \t P: 0.0108722151826\n",
      "Word: shadow \t P: 0.01087130736\n",
      "Word: man \t P: 0.0108712665318\n",
      "Word: men \t P: 0.0108710463031\n",
      "Word: larg \t P: 0.0108709133041\n",
      "Word: power \t P: 0.0108708730862\n",
      "Word: small \t P: 0.0108705278926\n",
      "Word: wall \t P: 0.0108704816025\n",
      "Word: believ \t P: 0.0108703006188\n",
      "Word: trick \t P: 0.0108702816478\n",
      "\n",
      "Topic # 1\n",
      "Word: easi \t P: 0.0901570116189\n",
      "Word: would \t P: 0.0492453338611\n",
      "Word: drunk \t P: 0.0491762203198\n",
      "Word: everyon \t P: 0.0491762126206\n",
      "Word: time \t P: 0.0491555088433\n",
      "Word: ash \t P: 0.00819693015352\n",
      "Word: dead \t P: 0.00819691438544\n",
      "Word: countri \t P: 0.00819671602037\n",
      "Word: see \t P: 0.00819671529887\n",
      "Word: burn \t P: 0.00819671524935\n",
      "\n",
      "Topic # 2\n",
      "Word: men \t P: 0.0577619756655\n",
      "Word: life \t P: 0.0397115504282\n",
      "Word: like \t P: 0.0397115489245\n",
      "Word: live \t P: 0.0397114922533\n",
      "Word: famili \t P: 0.0397114900503\n",
      "Word: lannist \t P: 0.0397114075915\n",
      "Word: shadow \t P: 0.0397110215108\n",
      "Word: resid \t P: 0.0397107178706\n",
      "Word: knock \t P: 0.021660568958\n",
      "Word: sneer \t P: 0.0216605684721\n",
      "\n",
      "Topic # 3\n",
      "Word: time \t P: 0.0474232045567\n",
      "Word: hand \t P: 0.0474121559527\n",
      "Word: ash \t P: 0.0258766179678\n",
      "Word: think \t P: 0.0258659905408\n",
      "Word: dothraki \t P: 0.0258610169915\n",
      "Word: son \t P: 0.0258610166055\n",
      "Word: khal \t P: 0.0258610162854\n",
      "Word: great \t P: 0.0258610162249\n",
      "Word: carri \t P: 0.0258610162051\n",
      "Word: last \t P: 0.0258610160855\n",
      "\n",
      "Topic # 4\n",
      "Word: climb \t P: 0.0521196734073\n",
      "Word: tri \t P: 0.0358322148125\n",
      "Word: ladder \t P: 0.035832214102\n",
      "Word: chao \t P: 0.0358322139514\n",
      "Word: dead \t P: 0.0195486225536\n",
      "Word: god \t P: 0.0195447364352\n",
      "Word: break \t P: 0.0195447362757\n",
      "Word: real \t P: 0.0195447362035\n",
      "Word: mani \t P: 0.0195447361874\n",
      "Word: love \t P: 0.0195447361651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing topic modeling result - Tuple (word, probability): The most probable words in topic \"topicid\" with probabiliity\n",
    "print 'The most probable word in topic \"Topic Id\" with Probability \\n'\n",
    "i = 0\n",
    "for topic in lda_model.show_topics(num_topics = 10, formatted = False):\n",
    "    print \"Topic # \" + str(i) + \"\\n\",\n",
    "    for word, prob in topic[1]:\n",
    "        print \"Word: {} \\t P: {}\".format(word, prob)\n",
    "    print \"\"\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (bigram)\n",
    "**Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample documents (Quotes from Game of Thrones T.V. Series)\n",
    "doc_0 = \"It is rare to meet a Lannister who share my enthusiasm for dead Lannisters.\"\n",
    "doc_1 = \"Arya Stark hasn't been seen since her father was killed. Where do you think she is? My money's on dead.\\\n",
    "         There's a certain safety in death, wouldn't you say?\"\n",
    "doc_2 = \"It's not easy being drunk all the time. If it were easy, everyone would do it.\"\n",
    "doc_3 = \"He would see this country burn if he could be king of the ashes.\"\n",
    "doc_4 = \"I will hurt you for this. A day will come when you think you are safe and happy, and your joy will turn to \\\n",
    "         ashes in your mouth. And you will know the debt is paid.\"\n",
    "doc_5 = \"All my life men like you have sneered at me. And all my life I've been knocking men like you into the dust.\"\n",
    "doc_6 = \"Power resides where men believe it resides. It's a trick, a shadow on the wall. And a very small man can \\\n",
    "         cast a very large shadow.\"\n",
    "doc_7 = \"It's the family name that lives on. It's all that lives on. Not your personal glory, not your honor...but family.\"\n",
    "doc_8 = \"I am a Khaleesi of the Dothraki. I am the wife of the great Khal and I carry his son inside me. The next time \\\n",
    "         you raise a hand to me will be the last time you have hands.\"\n",
    "doc_9 = \"Chaos isn't a pit. Chaos is a ladder. Many who try to climb it fail and never get to try again. \\\n",
    "         The fall breaks them. And some are given a chance to climb, they cling to the realm or the gods or love. \\\n",
    "         only the ladder is real. The climb is all there is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word2Vec\n",
    "\n",
    "- Word2Vec attempts to associate words with points in space\n",
    "- Spatial distance between words then describes the relation (similarity) between these words\n",
    "- Words that are spatially close are similar\n",
    "- Words are represented by continuous vector over $x$ dimensions\n",
    "- Word2Vec is a very simple neural network with a single hidden layer\n",
    "![w2v](images/w2v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displacement vector (the vector between 2 vectors) describes the relation between 2 words\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFVCAYAAADVDycqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119237250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2 Dimensional Example\n",
    "words = ['queen', 'book', 'king', 'magazine', 'car', 'bike']\n",
    "vectors = np.array([[0.1, 0.3],    # queen\n",
    "                    [-0.5, -0.1],  # book\n",
    "                    [0.2, 0.2],    # king\n",
    "                    [-0.3, -0.2],  # magazine\n",
    "                    [-0.5, 0.4],   # car\n",
    "                    [-0.45, 0.3]]) # bike\n",
    "\n",
    "# Plot the vectors\n",
    "sb.plt.plot(vectors[:,0], vectors[:,1], 'o')\n",
    "sb.plt.xlim(-0.6, 0.3)\n",
    "sb.plt.ylim(-0.3, 0.5)\n",
    "for word, x, y in zip(words, vectors[:,0], vectors[:,1]):\n",
    "    sb.plt.annotate(word, (x, y), size = 12)\n",
    "    \n",
    "print 'Displacement vector (the vector between 2 vectors) describes the \\\n",
    "relation between 2 words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1190ef790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEeCAYAAACZlyICAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118b04f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What is the probability of the following words given Cinderella, i.e. P(w|Cinderella)\n",
    "s = pd.Series([0.1, 0.4, 0.01, 0.2, 0.05],\n",
    "              index = ['pumpkin', 'shoe', 'tree', 'prince', 'luck'])\n",
    "\n",
    "s.plot(kind = 'bar')\n",
    "sb.plt.ylabel('$P(w|Cindrella)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax Regression\n",
    "sentences = ['the king loves the queen', 'the queen loves the king',\n",
    "             'the dwarf hates the king', 'the queen hates the dwarf',\n",
    "             'the dwarf poisons the king', 'the dwarf poisons the queen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to create vocabulary (a dictionary)\n",
    "def Vocabulary():\n",
    "    dictionary = defaultdict()\n",
    "    dictionary.default_factory = lambda: len(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "# Function to transform a list of strings into a list of lists\n",
    "# where each unique item is converted into a unique integer\n",
    "def docs2bow(docs, dictionary):\n",
    "    for doc in docs:\n",
    "        yield [dictionary[word] for word in doc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 0, 3],\n",
       " [0, 3, 2, 0, 1],\n",
       " [0, 4, 5, 0, 1],\n",
       " [0, 3, 5, 0, 4],\n",
       " [0, 4, 6, 0, 1],\n",
       " [0, 4, 6, 0, 3]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = Vocabulary()\n",
    "sentences_bow = list(docs2bow(sentences, vocabulary))\n",
    "sentences_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct the 2 matrices W and W'\n",
    "V, N = len(vocabulary), 3\n",
    "WI = (np.random.random((V, N)) - 0.5)/N\n",
    "WO = (np.random.random((N, V)) - 0.5)/V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01755287  0.00948371 -0.03769007]\n",
      " [ 0.14400452  0.07107485 -0.09754199]\n",
      " [-0.11083702 -0.16365717  0.16403924]\n",
      " [-0.15566368 -0.16157512 -0.02783417]\n",
      " [-0.14974128 -0.02036993 -0.13288651]\n",
      " [-0.05167142  0.03209973 -0.10499077]\n",
      " [ 0.04482381  0.03332321  0.03842618]]\n",
      "[[ 0.01939817  0.02451541 -0.05226995 -0.06998994 -0.06094672 -0.04398479\n",
      "  -0.01807224]\n",
      " [ 0.04687407 -0.00189492  0.03760058 -0.05221544 -0.01962851 -0.00782222\n",
      "   0.00963694]\n",
      " [ 0.00909789  0.01019005 -0.00968965  0.00420316 -0.04044071 -0.0204089\n",
      "   0.06419813]]\n"
     ]
    }
   ],
   "source": [
    "print WI\n",
    "print WO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0094577441849\n"
     ]
    }
   ],
   "source": [
    "print np.dot(WI[vocabulary['dwarf']], WO.T[vocabulary['hates']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.142857142857\n"
     ]
    }
   ],
   "source": [
    "p_hates_dwarf = (np.exp(np.dot(WI[vocabulary['dwarf']], WO.T[vocabulary['hates']]))/\n",
    "                sum(np.exp(np.dot(WI[vocabulary['dwarf']], WO.T[vocabulary['hates']]))\n",
    "                   for w in vocabulary))\n",
    "print p_hates_dwarf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample documents (Quotes from Game of Thrones T.V. Series)\n",
    "doc_0 = \"It is rare to meet a Lannister who share my enthusiasm for dead Lannisters.\"\n",
    "doc_1 = \"Arya Stark hasn't been seen since her father was killed. Where do you think she is? My money's on dead.\\\n",
    "         There's a certain safety in death, wouldn't you say?\"\n",
    "doc_2 = \"It's not easy being drunk all the time. If it were easy, everyone would do it.\"\n",
    "doc_3 = \"He would see this country burn if he could be king of the ashes.\"\n",
    "doc_4 = \"I will hurt you for this. A day will come when you think you are safe and happy, and your joy will turn to \\\n",
    "         ashes in your mouth. And you will know the debt is paid.\"\n",
    "doc_5 = \"All my life men like you have sneered at me. And all my life I've been knocking men like you into the dust.\"\n",
    "doc_6 = \"Power resides where men believe it resides. It's a trick, a shadow on the wall. And a very small man can \\\n",
    "         cast a very large shadow.\"\n",
    "doc_7 = \"It's the family name that lives on. It's all that lives on. Not your personal glory, not your honor...but family.\"\n",
    "doc_8 = \"I am a Khaleesi of the Dothraki. I am the wife of the great Khal and I carry his son inside me. The next time \\\n",
    "         you raise a hand to me will be the last time you have hands.\"\n",
    "doc_9 = \"Chaos isn't a pit. Chaos is a ladder. Many who try to climb it fail and never get to try again. \\\n",
    "         The fall breaks them. And some are given a chance to climb, they cling to the realm or the gods or love. \\\n",
    "         only the ladder is real. The climb is all there is.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of sample documents\n",
    "doc_list = [doc_0, doc_1, doc_2, doc_3, doc_4, doc_5, doc_6, doc_7, doc_8, doc_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform vectorization \n",
    "tfidf_vectorizer = TfidfVectorizer(doc_list, stop_words='english')\n",
    "\n",
    "# Extract tf-idf features for NMF\n",
    "tfidf = tfidf_vectorizer.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'arya', u'ashes', u'believe', u'breaks', u'burn', u'carry', u'cast', u'certain', u'chance', u'chaos', u'climb', u'cling', u'come', u'country', u'day', u'dead', u'death', u'debt', u'dothraki', u'drunk', u'dust', u'easy', u'enthusiasm', u'fail', u'fall', u'family', u'father', u'given', u'glory', u'gods', u'great', u'hand', u'hands', u'happy', u'hasn', u'honor', u'hurt', u'inside', u'isn', u'joy', u'khal', u'khaleesi', u'killed', u'king', u'knocking', u'know', u'ladder', u'lannister', u'lannisters', u'large', u'life', u'like', u'lives', u'love', u'man', u'meet', u'men', u'money', u'mouth', u'paid', u'personal', u'pit', u'power', u'raise', u'rare', u'real', u'realm', u'resides', u'safe', u'safety', u'say', u'seen', u'shadow', u'share', u'small', u'sneered', u'son', u'stark', u'think', u'time', u'trick', u'try', u'turn', u've', u'wall', u'wife', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocab = tfidf_vectorizer.get_feature_names()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the NMF model - Nonnegative Double Singular Value Decomposition initialization\n",
    "num_topics = 5\n",
    "nmf = NMF(n_components = num_topics, init = 'nndsvd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Document topic\n",
    "doc_topic = nmf.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "topic_words = []\n",
    "for topic in nmf.components_:\n",
    "    idx = np.argsort(topic)[::-1][0:n_top_words]\n",
    "    topic_words.append([vocab[i] for i in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "easy time drunk raise dothraki wife inside hands hand great\n",
      "Topic #1:\n",
      "ashes king burn country happy paid joy debt safe day\n",
      "Topic #2:\n",
      "men like life shadow resides sneered dust ve knocking believe\n",
      "Topic #3:\n",
      "dead enthusiasm lannister lannisters meet share rare wouldn say certain\n",
      "Topic #4:\n",
      "climb try ladder chaos isn pit given real realm love\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf, vocab, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization\n",
    "\n",
    "[Differences](http://nbviewer.jupyter.org/github/dolaameng/tutorials/blob/master/topic-finding-for-short-texts/topics_for_short_texts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple K-Means Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample documents (Quotes from Game of Thrones T.V. Series)\n",
    "doc_0 = \"It is rare to meet a Lannister who share my enthusiasm for dead Lannisters.\"\n",
    "doc_1 = \"Arya Stark hasn't been seen since her father was killed. Where do you think she is? My money's on dead.\\\n",
    "         There's a certain safety in death, wouldn't you say?\"\n",
    "doc_2 = \"It's not easy being drunk all the time. If it were easy, everyone would do it.\"\n",
    "doc_3 = \"He would see this country burn if he could be king of the ashes.\"\n",
    "doc_4 = \"I will hurt you for this. A day will come when you think you are safe and happy, and your joy will turn to \\\n",
    "         ashes in your mouth. And you will know the debt is paid.\"\n",
    "doc_5 = \"All my life men like you have sneered at me. And all my life I've been knocking men like you into the dust.\"\n",
    "doc_6 = \"Power resides where men believe it resides. It's a trick, a shadow on the wall. And a very small man can \\\n",
    "         cast a very large shadow.\"\n",
    "doc_7 = \"It's the family name that lives on. It's all that lives on. Not your personal glory, not your honor...but family.\"\n",
    "doc_8 = \"I am a Khaleesi of the Dothraki. I am the wife of the great Khal and I carry his son inside me. The next time \\\n",
    "         you raise a hand to me will be the last time you have hands.\"\n",
    "doc_9 = \"Chaos isn't a pit. Chaos is a ladder. Many who try to climb it fail and never get to try again. \\\n",
    "         The fall breaks them. And some are given a chance to climb, they cling to the realm or the gods or love. \\\n",
    "         only the ladder is real. The climb is all there is.\"\n",
    "\n",
    "# Create a list of sample documents\n",
    "doc_list = [doc_0, doc_1, doc_2, doc_3, doc_4, doc_5, doc_6, doc_7, doc_8, doc_9]\n",
    "\n",
    "# TFIDF vectorizer\n",
    "vectorizer = TfidfVectorizer(decode_error='replace', stop_words='english')\n",
    "\n",
    "# TFIDF fit and transform\n",
    "tfidf = vectorizer.fit_transform(doc_list)\n",
    "\n",
    "# KMeans model\n",
    "model = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, precompute_distances='auto')\n",
    "\n",
    "# Fit model\n",
    "model.fit(tfidf)\n",
    "\n",
    "# Print top terms per cluster\n",
    "terms = vectorizer.get_feature_names()\n",
    "ordered_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "print \"Top 10 terms per cluster:\"\n",
    "for i in range(3):\n",
    "    print \"Cluster # {}\".format(i)\n",
    "    for idx in ordered_centroids[i, :10]:\n",
    "        print \"Terms: {}\".format(terms[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gensim\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample documents (Quotes from Game of Thrones T.V. Series)\n",
    "doc_0 = \"It is rare to meet a Lannister who share my enthusiasm for dead Lannisters.\"\n",
    "doc_1 = \"Arya Stark hasn't been seen since her father was killed. Where do you think she is? My money's on dead.\\\n",
    "         There's a certain safety in death, wouldn't you say?\"\n",
    "doc_2 = \"It's not easy being drunk all the time. If it were easy, everyone would do it.\"\n",
    "doc_3 = \"He would see this country burn if he could be king of the ashes.\"\n",
    "doc_4 = \"I will hurt you for this. A day will come when you think you are safe and happy, and your joy will turn to \\\n",
    "         ashes in your mouth. And you will know the debt is paid.\"\n",
    "doc_5 = \"All my life men like you have sneered at me. And all my life I've been knocking men like you into the dust.\"\n",
    "doc_6 = \"Power resides where men believe it resides. It's a trick, a shadow on the wall. And a very small man can \\\n",
    "         cast a very large shadow.\"\n",
    "doc_7 = \"It's the family name that lives on. It's all that lives on. Not your personal glory, not your honor...but family.\"\n",
    "doc_8 = \"I am a Khaleesi of the Dothraki. I am the wife of the great Khal and I carry his son inside me. The next time \\\n",
    "         you raise a hand to me will be the last time you have hands.\"\n",
    "doc_9 = \"Chaos isn't a pit. Chaos is a ladder. Many who try to climb it fail and never get to try again. \\\n",
    "         The fall breaks them. And some are given a chance to climb, they cling to the realm or the gods or love. \\\n",
    "         only the ladder is real. The climb is all there is.\"\n",
    "\n",
    "# Create a list of sample documents\n",
    "doc_list = [doc_0, doc_1, doc_2, doc_3, doc_4, doc_5, doc_6, doc_7, doc_8, doc_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Function to clean text and modify string\n",
    "    Process: lowercase > remove punctuation > tokenize > stopword removal \n",
    "        Output: cleaned and modified text string\n",
    "    '''\n",
    "    # RegExp tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # English stopwords\n",
    "    en_stopwords = stopwords.words('english')\n",
    "    \n",
    "    # Convert text to lower case\n",
    "    raw_text = text.lower()\n",
    "    # Remove punctuations\n",
    "    raw_text_punct_removed = raw_text.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(raw_text_punct_removed)\n",
    "    # Remove stopwords: English only\n",
    "    tokens_remaining = [t for t in tokens if t not in en_stopwords]    \n",
    "    return tokens_remaining\n",
    "\n",
    "# Clean the texts\n",
    "doc_clean = []\n",
    "for i in doc_list:\n",
    "    doc_clean.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:consider setting layer size to a multiple of 4 for greater performance\n",
      "WARNING:gensim.models.word2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "model = gensim.models.Word2Vec(doc_clean, min_count=1, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'lannister':  [('arya', 0.6569440364837646), ('certain', 0.6248986721038818), ('dothraki', 0.5493209362030029), ('like', 0.523289680480957), ('wife', 0.5125791430473328), ('breaks', 0.49835172295570374), ('last', 0.47300979495048523), ('chaos', 0.4492312967777252), ('name', 0.4115704298019409), ('safety', 0.38567376136779785)]\n",
      "\n",
      "\n",
      "Word that doesn't match:  khal\n",
      "\n",
      "\n",
      "Word similarity:  -0.523537026102\n",
      "\n",
      "\n",
      "Raw Vector of a word:  [-0.03621156 -0.02768207 -0.02227548  0.00076247 -0.01228873 -0.03467362\n",
      "  0.03842506 -0.04867595  0.0058341   0.01725009]\n"
     ]
    }
   ],
   "source": [
    "# Most similar Words\n",
    "print \"Most similar words to 'lannister': \", model.most_similar('lannister')\n",
    "print '\\n'\n",
    "\n",
    "# Words that Doesn't Match\n",
    "print \"Word that doesn't match: \", model.doesnt_match(\"arya king khal lannister\".split())\n",
    "print '\\n'\n",
    "\n",
    "# Similarity\n",
    "print \"Word similarity: \", model.similarity('stark', 'lannister')\n",
    "print '\\n'\n",
    "\n",
    "# Raw Vector of a word\n",
    "print \"Raw Vector of a word: \", model['khal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 99 words\n",
      "Word Vector length (# of features):  10\n"
     ]
    }
   ],
   "source": [
    "# Feature vector of each word in vocabulary\n",
    "print \"Vocabulary: {} words\".format(model.syn0.shape[0])\n",
    "print \"Word Vector length (# of features): \", model.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector averaging \n",
    "\n",
    "Take individual word vectors and transform them into a feature set that is the same length for each document or paragraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    '''\n",
    "    Function to average all of word vectors in a given paragraph\n",
    "    '''\n",
    "    # Pre-initialize an empty numpy array for speed\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    \n",
    "    n_words = 0\n",
    "    \n",
    "    # Index2word is a list that contains the name of words in models vocabulary\n",
    "    index2word_set = set(model.index2word) # Converting to set for speed\n",
    "    \n",
    "    # Loop over each word in document/paragraph, if it is in models vocabulary\n",
    "    # add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words = n_words + 1\n",
    "            feature_vec = np.add(feature_vec, n_words)\n",
    "            \n",
    "    # Divide the result by the number of words to get the average\n",
    "    feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "    \n",
    "def average_feature_vecs(docs, model, num_features):\n",
    "    '''\n",
    "    Given a set of documents (each document is a list of words), calculate\n",
    "    the average feature vector for each one and return a numpy 2d array\n",
    "    '''\n",
    "    # Initialize a counter\n",
    "    counter = 0\n",
    "    \n",
    "    # Pre-initialize an empty 2D numpy array for speed\n",
    "    doc_feature_vecs = np.zeros((len(docs), num_features), dtype=\"float32\")\n",
    "    \n",
    "    # Loop through the documents and get average feature vec\n",
    "    for doc in docs:\n",
    "        # Call make feature vector function\n",
    "        doc_feature_vecs[counter] = make_feature_vec(doc, model, num_features)\n",
    "        # Increment the counter\n",
    "        counter = counter + 1\n",
    "    return doc_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.    4.    4.    4.    4.    4.    4.    4.    4.    4. ]\n",
      " [  8.5   8.5   8.5   8.5   8.5   8.5   8.5   8.5   8.5   8.5]\n",
      " [  3.5   3.5   3.5   3.5   3.5   3.5   3.5   3.5   3.5   3.5]\n",
      " [  4.    4.    4.    4.    4.    4.    4.    4.    4.    4. ]\n",
      " [  7.    7.    7.    7.    7.    7.    7.    7.    7.    7. ]\n",
      " [  5.5   5.5   5.5   5.5   5.5   5.5   5.5   5.5   5.5   5.5]\n",
      " [  7.    7.    7.    7.    7.    7.    7.    7.    7.    7. ]\n",
      " [  4.5   4.5   4.5   4.5   4.5   4.5   4.5   4.5   4.5   4.5]\n",
      " [  8.    8.    8.    8.    8.    8.    8.    8.    8.    8. ]\n",
      " [ 12.5  12.5  12.5  12.5  12.5  12.5  12.5  12.5  12.5  12.5]]\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Get average feature vector\n",
    "doc_vecs = average_feature_vecs(doc_clean, model, 10)\n",
    "print doc_vecs\n",
    "print doc_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0\n",
      "['isnt', 'personal', 'enthusiasm', 'safe', 'shadow', 'large', 'small', 'see', 'country', 'could', 'knocking', 'carry', 'raise', 'stark', 'next', 'lives', 'resides', 'glory', 'theres', 'ive', 'certain', 'say', 'seen', 'realm', 'hand', 'mouth', 'man']\n",
      "Cluster: 1\n",
      "['gods', 'fall', 'death', 'father', 'killed', 'real', 'know', 'hands', 'like', 'everyone', 'dead', 'try', 'since', 'burn', 'khaleesi', 'power', 'joy', 'men', 'debt', 'come', 'great', 'rare', 'khal', 'wouldnt', 'climb', 'think', 'love', 'family', 'son', 'happy', 'life', 'hurt', 'king', 'inside', 'trick', 'many', 'meet', 'pit', 'paid', 'dust', 'time']\n",
      "Cluster: 2\n",
      "['lannister', 'easy', 'get', 'breaks', 'day', 'name', 'dothraki', 'ashes', 'arya', 'honorbut', 'fail', 'chaos', 'safety', 'lannisters', 'turn', 'drunk', 'wall', 'given', 'would', 'moneys', 'last', 'believe', 'cast', 'share', 'hasnt', 'cling', 'ladder', 'chance', 'never', 'wife', 'sneered']\n"
     ]
    }
   ],
   "source": [
    "# Word vectors\n",
    "word_vectors = model.syn0\n",
    "\n",
    "# Initialize k-means object and extract centroids\n",
    "k_means_word2vec = KMeans(n_clusters=3)\n",
    "idx = k_means_word2vec.fit_predict(word_vectors)\n",
    "\n",
    "# Create a dictionary\n",
    "word_centroid_map = dict(zip(model.index2word, idx))\n",
    "\n",
    "#\n",
    "for cluster in range(0, 3):\n",
    "    print \"Cluster: {}\".format(cluster)\n",
    "    # Find all words in cluster i\n",
    "    words = []\n",
    "    for i in range(0, len(word_centroid_map.values())):\n",
    "        if word_centroid_map.values()[i] == cluster:\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
