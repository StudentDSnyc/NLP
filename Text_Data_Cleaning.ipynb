{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apostrophe Modification\n",
    "- [Code modified from \"Multiple word replace in text using dictionary\"](https://www.daniweb.com/programming/software-development/code/216636/multiple-word-replace-in-text-python)\n",
    "- Drawback: Need to create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace words in a text that match key strings in a dict with value strings\n",
    "import re\n",
    "\n",
    "def word_replace(text, word_dict):\n",
    "    # Compile a regular expression pattern into a regular expression object\n",
    "    # re.escape(string): Return string with all non-alphanumerics backslashed\n",
    "    # map(function, iterable): Apply function to every item of iterable and return list of results\n",
    "    pattern = re.compile('|'.join(map(re.escape, word_dict)))\n",
    "    \n",
    "    # Define a function to return value from dictionary based on matching\n",
    "    def translate(match):\n",
    "        #print match.group()\n",
    "        return word_dict[match.group()]\n",
    "    \n",
    "    return pattern.sub(translate, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  That's not possible. I'll see if they're coming. I'm sure they wouldn't mind coming\n",
      "Modified text:  That is not possible. I will see if they are coming. I am sure they would not mind coming\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary\n",
    "apostrophes = {\"'s\": ' is', \"'re\": ' are', \"'ll\": ' will', \"n't\": ' not', \"'m\": ' am'}\n",
    "\n",
    "# Text example\n",
    "text = \"That's not possible. I'll see if they're coming. I'm sure they wouldn't mind coming\"\n",
    "print 'Original text: ', text\n",
    "\n",
    "modified_text = word_replace(text, apostrophes)\n",
    "print 'Modified text: ', modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  That's not possible. I'll see if they're coming. I'm sure they wouldn't mind coming\n",
      "Modified text:  That is not possible. I will see if they are coming. I am sure they would not mind coming\n"
     ]
    }
   ],
   "source": [
    "# Another approach: Is it good?\n",
    "def replace_word(text, word_dict):\n",
    "    for key in word_dict:\n",
    "        text = text.replace(key, word_dict[key])\n",
    "    return text\n",
    "\n",
    "# Create a dictionary\n",
    "apostrophes = {\"'s\": ' is', \"'re\": ' are', \"'ll\": ' will', \"n't\": ' not', \"'m\": ' am'}\n",
    "\n",
    "# Text example\n",
    "text = \"That's not possible. I'll see if they're coming. I'm sure they wouldn't mind coming\"\n",
    "print 'Original text: ', text\n",
    "\n",
    "modified_text = replace_word(text, apostrophes)\n",
    "print 'Modified text: ', modified_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  This $$notebook** is about ~text cleaning#\n",
      "Punctuations:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Punctuation free text:  This notebook is about text cleaning\n"
     ]
    }
   ],
   "source": [
    "# Text example\n",
    "text = \"This $$notebook** is about ~text cleaning#\"\n",
    "print 'Original text: ', text\n",
    "\n",
    "import string\n",
    "\n",
    "# string.punctuation is simply a list of punctuation\n",
    "output = text.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "\n",
    "print 'Punctuations: ', string.punctuation\n",
    "print 'Punctuation free text: ', output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "maketrans() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7f5bf630a2ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use of makettrans to tokenize on spaces, stripping punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#[s.translate(table) for s in text.split(' ') if s != '']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: maketrans() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "# use of makettrans to tokenize on spaces, stripping punctuation\n",
    "table = string.maketrans({ch: None for ch in string.punctuation})\n",
    "\n",
    "#[s.translate(table) for s in text.split(' ') if s != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Diacritical Marks (accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JesucristÕ. Mas el herido fue por nuestras rebeliones, mÕlido\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'JesucristO. Mas el herido fue por nuestras rebeliones, mOlido'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "text = 'JesucristÕ. Mas el herido fue por nuestras rebeliones, mÕlido'\n",
    "\n",
    "unicode_text = unicode(text, \"utf-8\")\n",
    "print unicode_text\n",
    "\n",
    "def remove_diacritic(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii\n",
    "\n",
    "remove_diacritic(unicode_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction <font color=steelblue>(works with Tokenized text)</font>\n",
    "- Peter Norvig's [Algorithm](http://norvig.com/spell-correct.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Code: Peter Norvig's Spelling Correction Algorithm ####\n",
    "# Import libraries\n",
    "import re\n",
    "import collections\n",
    "\n",
    "# Function to convert text to lower case\n",
    "def words(text): return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "# Function to count how many times each word occurs (i.e. train a Bayesian probability model)\n",
    "def train(features):\n",
    "    # Create empty dict that will assign value = 1 to nonexistent key\n",
    "    model = collections.defaultdict(lambda: 1) # 1 is used for smoothing due to new words\n",
    "    for f in features:\n",
    "        model[f] += 1 # Update count \n",
    "    return model\n",
    "\n",
    "# Store \n",
    "NWORDS = train(words(file('data/big.txt').read()))\n",
    "\n",
    "# English alphabets\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "# Edit Distance = 1 (According to literature: 80 to 90% of spelling errors are an edit distance of 1 from the target)\n",
    "def edit_dist_1(word):\n",
    "    # Split the word iteratively and create a list\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    # Delete a letter from word splits in splits list\n",
    "    deletes = [a + b[1:] for a, b in splits if b]\n",
    "    # Transpose: Swap adjacent letters\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b) > 1]\n",
    "    # Replace: Change one letter to another\n",
    "    replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "    # Insert: Add a letter\n",
    "    inserts = [a + c + b for a, b in splits for c in alphabet]\n",
    "    # Return set\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "# ----------------------------\n",
    "# Edit Distance = 2 (Apply \"edit_dist_1\" function to all the results of \"edit_dist_1\" function)\n",
    "def edit_dist_2(word):\n",
    "    return set(e2 for e1 in edit_dist_1(word) for e2 in edit_dist_1(e1))\n",
    "# To avoid lot of computation - Optimize by keeping the candidates that are known words\n",
    "# ----------------------------\n",
    "\n",
    "# Edit Distance = 2 for known words given in \"big.txt\"\n",
    "def known_edit_dist_2(word):\n",
    "    return set(e2 for e1 in edit_dist_1(word) for e2 in edit_dist_1(e1) if e2 in NWORDS)\n",
    "\n",
    "# Known words in language model training data\n",
    "def known(words): return (set(w for w in words if w in NWORDS))\n",
    "\n",
    "# Function to return the element with highest P(candidate) i.e. how likely is candidate to appear in English\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edit_dist_1(word)) or known_edit_dist_2(word) or [word]\n",
    "    return max(candidates, key = NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'great', 'idea', 'doctor', 'cares', 'who']\n"
     ]
    }
   ],
   "source": [
    "# Spelling correction test\n",
    "words = ['thos', 'is', 'a', 'gread', 'idia', 'doktor', 'careus', 'whos']\n",
    "print [correct(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction <font color=steelblue>(works with Tokenized text)</font>\n",
    "- PyEnchant - [a spelling checking library for Python](http://pythonhosted.org/pyenchant/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iehp\n"
     ]
    }
   ],
   "source": [
    "#! pip install pyenchant\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Spell Checker Class: Taken from \"Python Text Processing with NLTK 2.0 Cookbook return word if word in the dictionary \n",
    "# else return closest word \n",
    "class SpellingReplacer(object):\n",
    "    def __init__(self, dict_name='en_US', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = 2\n",
    "    def replace(self, word):\n",
    "        # If word in US english dict return word\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        # If word not in US english dict then return the closest matched word\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        # Otherwise return word\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "# Quick test\n",
    "replacer = SpellingReplacer()\n",
    "print replacer.replace('iehp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thews', 'is', 'a', 'read', 'Lidia', 'doctor', 'cares', 'whews']\n"
     ]
    }
   ],
   "source": [
    "# Spelling correction test\n",
    "words = ['thos', 'is', 'a', 'gread', 'idia', 'doktor', 'careus', 'whos']\n",
    "print [replacer.replace(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-ASCII Characters\n",
    "\n",
    "- [Reference](http://stackoverflow.com/questions/4020539/process-escape-sequences-in-a-string-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text (non-ASCII):  Š«Ç‘¾_±Œ_Ä__Ä_îµ stop texting!\n",
      "Text ():  \\xc5\\xa0\\xc2\\x81\\xc2\\x90\\xc2\\x8d\\xc2\\xab\\xc3\\x87\\xc2\\x8d\\xe2\\x80\\x98\\xc2\\xbe_\\xc2\\xb1\\xc5\\x92_\\xc3\\x84__\\xc3\\x84_\\xc3\\xae\\xc2\\xb5 stop texting!\n",
      "Text (glyph replaced):   stop texting!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Text example with non-ASCII characters\n",
    "text = 'Š«Ç‘¾_±Œ_Ä__Ä_îµ stop texting!'\n",
    "\n",
    "print 'Text (non-ASCII): ', text\n",
    "print 'Text (): ', text.encode('string-escape')\n",
    "\n",
    "# Convert all non-ASCII characters into their two-digit \\xXX and four-digit \\uXXXX hexadecimal representations\n",
    "def replace_glyph_chars(text):\n",
    "    # Compile regular expression to search: \\xXX\n",
    "    glyph_chars_pattern = re.compile('\\\\\\\\x(\\w{2,4})')\n",
    "    # replacement: re.sub(pattern, repl, string)\n",
    "    temp = re.sub(glyph_chars_pattern, '', text.encode('string-escape'))\n",
    "    return temp\n",
    "\n",
    "print 'Text (glyph replaced): ', replace_glyph_chars(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Encoding and Unicode\n",
    "- [Pragmatic Unicode](http://nedbatchelder.com/text/unipain.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text without spaces into list of words (or Segment text)\n",
    "- [Algorithm based on Zipf's law](http://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words)\n",
    "- [Peter Norvig's Natural Language Corpus Data: Beautiful Data (Google's n-Gram word frequency lists and more)](http://norvig.com/ngrams/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Code: Peter Norvig's Text Segmentation Algorithm ####\n",
    "import operator\n",
    "\n",
    "# Define a \"Memoize function\"\n",
    "def memo(f):\n",
    "    \"Memoize function f\"\n",
    "    table = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in table:\n",
    "            table[args] = f(*args)\n",
    "        return table[args]\n",
    "    fmemo.memo = table\n",
    "    return fmemo\n",
    "\n",
    "# Word segmentation\n",
    "\n",
    "# Decorator\n",
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the best segmentation of text.\"\n",
    "    if not text: return []\n",
    "    candidates = ([first]+segment(remaining) for first, remaining in splits(text))\n",
    "    return max(candidates, key=probability_word_sequence)\n",
    "\n",
    "# Function to split text in all possible pairs\n",
    "def splits(text, L=20):\n",
    "    \"\"\"\n",
    "    Return a list of all possible (first, remaining) pairs,\n",
    "    where length of first <= L\n",
    "    Example: splits(\"aword\") \n",
    "    Output: [('a', 'word'), ('aw', 'ord'), ('awo', 'rd'), ('awor', 'd'), ('aword', '')]\n",
    "    \"\"\"\n",
    "    text_length = range(min(len(text), L))\n",
    "    return [(text[:i+1], text[i+1:]) for i in text_length]\n",
    "\n",
    "# Function to calculate Naive Bayes probability of a sequence of words\n",
    "def probability_word_sequence(words):\n",
    "    \"Return the Naive Bayes probability of a sequence of words.\"\n",
    "    return product(probability_word(w) for w in words)\n",
    "\n",
    "# Function to return product of a sequence of numbers.\n",
    "def product(numbers):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    # reduce(operator.mul, [1, 2, 3], 0.5) = 3.0\n",
    "    return reduce(operator.mul, numbers, 1)\n",
    "\n",
    "# Class to create a word probability distribution\n",
    "class probability_distribution(dict):\n",
    "    \"A probability distribution estimated from word counts in the datafile\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key, count in data:\n",
    "            # Set dictionary key's value by adding count\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        # Set value of N\n",
    "        self.N = float(N or sum(self.itervalues()))\n",
    "        # Set missing function\n",
    "        self.missingfn = missingfn or (lambda k, N: 1.0/N)\n",
    "    def __call__(self, key):\n",
    "        # If word in dictionary, then return probability of word\n",
    "        if key in self: return self[key]/self.N\n",
    "        # If word not in dictionary (i.e. unknown word), \n",
    "        # then return probability of unknown word calculated by missingfn\n",
    "        else: return self.missingfn(key, self.N)\n",
    "        \n",
    "# Function to estimate probability of an unknown word: missingfn\n",
    "def unknown_word_probability(unknown_word, N):\n",
    "    \"Estimate the probability of an unknown word\"\n",
    "    return 10.0/(N * 10**len(unknown_word))\n",
    "    \n",
    "#print unknown_word_probability('jokerisabadcharacterinthedarkknight', N)\n",
    "\n",
    "# Number of tokens: ~1 trillion in Google unigrams\n",
    "N = 1024908267229\n",
    "\n",
    "# Function to read 1/3 million most frequent words with counts\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read word, count pairs from file\"\n",
    "    for line in file(name):\n",
    "        yield line.split(sep)\n",
    "        \n",
    "probability_word = probability_distribution(datafile('data/count_1w.txt'), N, unknown_word_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'contact']\n"
     ]
    }
   ],
   "source": [
    "# Testing \"segment\"\n",
    "print segment('wouldcontact')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Emoji/Emoticon to Text\n",
    "- [Emoji data](http://ftp.unicode.org/Public/emoji/1.0/emoji-data.txt)\n",
    "- [Twitter data preprocessing](https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read emoji data\n",
    "f = open('data/emoji-data.txt', 'r')\n",
    "\n",
    "# Emoji dictionary\n",
    "emoji_dict = {}\n",
    "\n",
    "for line in f:\n",
    "    # Translate text (in a particular encoding) into unicode (i.e. decode) \n",
    "    line = line.decode('utf-8')\n",
    "    # Replace '(' and ')' with ';' in '(emojo character)'\n",
    "    line = re.sub('\\(', ';', line)\n",
    "    line = re.sub('\\)', ';', line)\n",
    "    # Split the line by ';'\n",
    "    line_token = line.split(';')\n",
    "    # 5th element is emoji character and 6th element is emoji description\n",
    "    emoji_dict[line_token[5]] = line_token[6].strip()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "# Note: To write out Unicode to a file or a terminal, we first need to translate it into a suitable encoding — \n",
    "# this translation out of Unicode is called encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords (English and Spanish) \n",
    "- <font color=steelblue>Works with Tokenized text</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English and Spanish stopwords:  466\n",
      "['notebook', 'text', 'cleaning']\n",
      "['text', 'cleaning']\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words: English and Spanish\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "sp_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Create a list of stopwords\n",
    "stop_words = []\n",
    "\n",
    "# Flattening a list of list\n",
    "[stop_words.extend(el) for el in [en_stopwords, sp_stopwords]] \n",
    "\n",
    "print 'Number of English and Spanish stopwords: ', len(stop_words)\n",
    "\n",
    "# Testing stopword removal\n",
    "text = ['this', 'is', 'a', 'notebook', 'about', 'text', 'cleaning']\n",
    "print [t for t in text if t not in stop_words]\n",
    "\n",
    "# Adding user defined custom stopwords\n",
    "custom_stopword = en_stopwords\n",
    "custom_stopword.append(\"notebook\")\n",
    "\n",
    "print [t for t in text if t not in custom_stopword]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- Regular Expression tokenizer\n",
    "- Word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  This notebook is about text cleaning!\n",
      "Tokenized text (regexp):  ['this', 'notebook', 'is', 'about', 'text', 'cleaning']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization - NLTK Regexp \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Testing Regexp tokenizer\n",
    "text = 'This notebook is about text cleaning!'\n",
    "print 'Text: ', text\n",
    "\n",
    "print 'Tokenized text (regexp): ', tokenizer.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "- [Measures](http://www.nltk.org/_modules/nltk/metrics/association.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['This', 'notebook', 'is', 'about', 'text', 'cleaning']\n",
      "<nltk.collocations.BigramCollocationFinder object at 0x109b7e510>\n",
      "[('This', 'notebook'), ('about', 'text'), ('is', 'about'), ('notebook', 'is'), ('text', 'cleaning')]\n",
      "['This', 'notebook', 'is', 'about', 'text', 'cleaning', ('This', 'notebook'), ('about', 'text'), ('is', 'about'), ('notebook', 'is'), ('text', 'cleaning')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "text = 'This notebook is about text cleaning!'\n",
    "\n",
    "# Tokenize text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print 'Tokens: ', tokens\n",
    "\n",
    "# Bigram Finder\n",
    "bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "print bigram_finder\n",
    "\n",
    "# Bigrams as per measure: Chi Square used here (more measures)\n",
    "bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 10)\n",
    "print bigrams\n",
    "\n",
    "# Join Tokens and Bigrams\n",
    "tokens = tokens + bigrams\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This notebook', 'notebook is', 'is about', 'about text', 'text cleaning']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "text = 'This notebook is about text cleaning!'\n",
    "# Tokenize text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "[' '.join(bigram) for bigram in bigrams(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['This', 'notebook', 'is', 'about', 'text', 'cleaning']\n",
      "<nltk.collocations.TrigramCollocationFinder object at 0x109b7e610>\n",
      "[('This', 'notebook', 'is'), ('about', 'text', 'cleaning'), ('is', 'about', 'text'), ('notebook', 'is', 'about')]\n",
      "['This', 'notebook', 'is', 'about', 'text', 'cleaning', ('This', 'notebook', 'is'), ('about', 'text', 'cleaning'), ('is', 'about', 'text'), ('notebook', 'is', 'about')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "text = 'This notebook is about text cleaning!'\n",
    "\n",
    "# Tokenize text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print 'Tokens: ', tokens\n",
    "\n",
    "# Bigram Finder\n",
    "trigram_finder = TrigramCollocationFinder.from_words(tokens)\n",
    "print trigram_finder\n",
    "\n",
    "# Bigrams as per measure: Chi Square used here (more measures)\n",
    "trigrams = trigram_finder.nbest(TrigramAssocMeasures.chi_sq, 10)\n",
    "print trigrams\n",
    "\n",
    "# Join Tokens and Bigrams\n",
    "tokens = tokens + trigrams\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This notebook is',\n",
       " 'notebook is about',\n",
       " 'is about text',\n",
       " 'about text cleaning']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import trigrams\n",
    "\n",
    "text = 'This notebook is about text cleaning!'\n",
    "# Tokenize text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "[' '.join(trigram) for trigram in trigrams(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Wordcloud\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot: https://eradiating.wordpress.com/tag/ipython-notebook/\n",
    "for i in range(tfidf_group.shape[0]):\n",
    "    words_ = tfidf_group.columns.values[1:]\n",
    "    weights = tfidf_group.ix[i, 1:].values\n",
    "    words_weights = zip(words_, weights)\n",
    "    label = 'Label-' + tfidf_group.ix[i, 'label']\n",
    "    words = ' '.join([x[0] for x in words_weights for times in\n",
    "                     range(0, int(x[1]*1000))])\n",
    "    wordcloud = WordCloud(background_color='black', \n",
    "                          width=1200, \n",
    "                          height=900).generate(words)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.title(label, fontsize = 24)\n",
    "    plt.tight_layout(pad=0)\n",
    "    wordcloud.to_file(label+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanford NER\n",
    "\n",
    "- **Issues with 7 Class** - Entities: Person, Location, Organization, Time, Date, Percent, Money and O: Other\n",
    "    - Often fails to detect a complete date. Example: Check \"Barack Hussein Obama was born on 4 August 1961\" on [NER Demo](http://nlp.stanford.edu:8080/ner/process)\n",
    "    - StanfordCoreNLP has SUTime (rule based Stanford Temporal Tagger) however SUTime is not used in Statistical NER directly\n",
    "    \n",
    "- **Stanford Temporal Tagger ([SUTime](http://nlp.stanford.edu/software/sutime.shtml))** - Rule based tagger returns (DATE, TIME, DURATION, and SET) tags\n",
    "    - Java based no Python \n",
    "    - [SUTime Demo](http://nlp.stanford.edu:8080/sutime/process)\n",
    "    - [english.sutime.txt](https://github.com/evandrix/stanford-corenlp/blob/master/sutime/english.sutime.txt)\n",
    "    - [english.holidays.sutime.txt](https://github.com/evandrix/stanford-corenlp/blob/master/sutime/english.holidays.sutime.txt)\n",
    "    - [defs.sutime.txt](https://github.com/evandrix/stanford-corenlp/blob/master/sutime/defs.sutime.txt)\n",
    "\n",
    "- **Text information extraction** - [Google Presentation](http://videolectures.net/mlas06_nigam_tie/ )  \n",
    "    \n",
    "```bash\n",
    "$ nano .bash_profile\n",
    "$ export JAVA_HOME=$(/usr/libexec/java_home) # Save and quit\n",
    "$ source .bash_profile \n",
    "$ echo $JAVA_HOME\n",
    "```\n",
    "\n",
    "```python\n",
    "# Set Java Path\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home'\n",
    "\n",
    "# Stanford NER\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "# Class and NER path: Move \"stanford-ner-2015-12-09\" to Project directory\n",
    "class_path = 'stanford-ner-2015-12-09/classifiers/english.muc.7class.distsim.crf.ser.gz'\n",
    "ner_path = 'stanford-ner-2015-12-09/stanford-ner.jar'\n",
    "\n",
    "# 7 Class NER Tagger\n",
    "# Entities: Person, Location, Organization, Time, Date, Percent, Money and O: Other\n",
    "st = StanfordNERTagger(class_path, ner_path)\n",
    "\n",
    "# Test\n",
    "test = 'John is studying at Stony Brook University in NY'\n",
    "tags = st.tag(test.split())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression Examples\n",
    "\n",
    "```python\n",
    "# Matching: 'a year', 'a month', 'two years ago', '3 years', '2.5 years'\n",
    "numbers = \"\"\"(^a(?=\\s)|one|two|three|four|five|six|seven|eight|nine|ten|eleven|\n",
    "             twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|\n",
    "             twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand)\"\"\"\n",
    "\n",
    "prefix = \"(about|almost|under|around|over|since)\"\n",
    "dmy = \"(year|day|week|month|yrs|yr|y)\"\n",
    "suffix = \"(before|after|earlier|later|ago)\"\n",
    "date1 = \"((\\d+(\\.\\d+)?|(\" + numbers + \"[-\\s]?))\\s*\" + dmy + \"s?\\s*(?:\" + suffix + \")?)\"\n",
    "print date1\n",
    "\n",
    "date1_reg = re.compile(date1, re.IGNORECASE|re.VERBOSE)\n",
    "\n",
    "cases = ['a year', '2.5 years', 'two years ago', 'nine months', 'eight yrs', 'four yr', 'five y', '5 years']\n",
    "for c in cases:\n",
    "    print date1_reg.findall(c)\n",
    "    \n",
    "\n",
    "# Matching: '2011', 'August 2011', 'Since 2012'\n",
    "year = \"((?<=\\s)\\d{4}|^\\d{4})\"\n",
    "date3_reg = re.compile(year)\n",
    "print year\n",
    "\n",
    "print date3_reg.findall('2011')\n",
    "print date3_reg.findall('May 2015')\n",
    "print date3_reg.findall('Since 2012')\n",
    "\n",
    "# Matching: 11/21/2015 or 11-21-2015\n",
    "date4 = \"\\d+[/-]\\d+[/-]\\d+\"\n",
    "date4_reg = re.compile(date4, re.IGNORECASE)\n",
    "print date4\n",
    "\n",
    "print date4_reg.findall('09/12/16')\n",
    "print date4_reg.findall('09-12-16')\n",
    "print date4_reg.findall('1/2/2016')\n",
    "print date4_reg.findall('1-2-2016')\n",
    "print date4_reg.findall('01/02/2016')\n",
    "print date4_reg.findall('01-02-2016')\n",
    "\n",
    "# Matching: Date in ISO format\n",
    "iso = \"\\d+[/-]\\d+[/-]\\d+ \\d+:\\d+:\\d+\\.\\d+\"\n",
    "iso_reg = re.compile(iso)\n",
    "print iso\n",
    "print iso_reg.findall(\"2016-07-07T21:16:16+00:00\n",
    "\n",
    "\n",
    "# Matching: 'Several years', 'few months', 'almost 3 years'\n",
    "prefix = \"(almost|about|under|around|over|this|next|last|few|several)\"\n",
    "dmy = \"(year|day|week|month|yrs|yr|y|mo|wk)\"\n",
    "date5 = \"((?:\" + prefix + ')?\\s*(?:(\\d+))?\\s*' +  '(?:' + dmy + \"s?\" + ')?)' \n",
    "\n",
    "date5_reg = re.compile(date5, re.IGNORECASE)\n",
    "print date5_reg.findall('several years')\n",
    "print date5_reg.findall('few weeks')\n",
    "print date5_reg.findall('almost 4 years')\n",
    "\n",
    "# Example of ignoring a group\n",
    "r = '(?:([a-z]{2,})_)?(\\d+)_([a-z]{2,}\\d+)_(\\d+)$'\n",
    "print r\n",
    "temp_reg = re.compile(r, re.IGNORECASE)\n",
    "\n",
    "x = 'SH_6208069141055_BC000388_20110412101855'\n",
    "y = '6208069141055_BC000388_20110412101855'\n",
    "\n",
    "print temp_reg.findall(x)\n",
    "print temp_reg.findall(y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example\n",
    "string = 'foobarbarfoo'\n",
    "\n",
    "# Look ahead positive: Finds the 1st \"bar\" which has \"bar\" after it\n",
    "pla = 'bar(?=bar)' \n",
    "re_1 = re.compile(pla)\n",
    "print re_1.findall(string)\n",
    "\n",
    "# Look ahead negative: Finds the 2nd \"bar\" which does not have \"bar\" after it\n",
    "nla = 'bar(?!bar)'\n",
    "re_2 = re.compile(nla)\n",
    "print re_2.findall(string)\n",
    "\n",
    "# Look behind positive: Finds the 1st \"bar\" which has \"foo\" before it\n",
    "plb = '(?<=foo)bar'\n",
    "re_3 = re.compile(plb)\n",
    "print re_3.findall(string)\n",
    "\n",
    "# Look behind negative: Finds the 2nd \"bar\" which does not have \"foo\" before it\n",
    "nlb = '(?<!foo)bar'\n",
    "re_4 = re.compile(nlb)\n",
    "print re_4.findall(string)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regular Expressions\n",
    "\n",
    "- [Regular Expression Tutorial](http://regexone.com/lesson/introduction_abcs)\n",
    "- [RegExr Tool](http://regexr.com/)\n",
    "- [RegEx101 (awesome)](https://regex101.com/)\n",
    "- [Automate boring stuff blog](https://automatetheboringstuff.com/chapter7/)\n",
    "- [Rexegg](http://www.rexegg.com/)\n",
    "- [Regular Expression Cheatsheet](http://www.pnotepad.org/docs/search/regular_expressions/)\n",
    "- [Look ahead/Look behind](http://stackoverflow.com/questions/2973436/regex-lookahead-lookbehind-and-atomic-groups)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Imports\n",
    "import re\n",
    "import dateparser\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Predefined strings.\n",
    "numbers = \"(^a(?=\\s)|one|two|three|four|five|six|seven|eight|nine|ten| \\\n",
    "          eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen| \\\n",
    "          eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty| \\\n",
    "          ninety|hundred|thousand)\"\n",
    "month = \"(january|february|march|april|may|june|july|august|september| \\\n",
    "          october|november|december)\"\n",
    "month_short = \"(jan|feb|mar|apr|may|jun|jul|aug|sept|oct|nov|dec)\"\n",
    "prefix = \"(couple|almost|about|under|around|over|this|next|last|few|several|since|many|some|number|handful|plenty|various)\"\n",
    "suffix = \"(before|after|earlier|later|ago|winter|spring|summer|fall)\"\n",
    "dwmy = \"(year|day|week|month|yrs|yr|y|wk)\"\n",
    "number_map = {\n",
    "    \"one\": 1,\n",
    "    \"two\": 2,\n",
    "    \"three\": 3,\n",
    "    \"four\": 4,\n",
    "    \"five\": 5,\n",
    "    \"six\": 6,\n",
    "    \"seven\": 7,\n",
    "    \"eight\": 8,\n",
    "    \"nine\": 9,\n",
    "    \"ten\": 10,\n",
    "    \"eleven\": 11,\n",
    "    \"twelve\": 12,\n",
    "    \"thirteen\": 13,\n",
    "    \"fourteen\": 14,\n",
    "    \"fifteen\": 15,\n",
    "    \"sixteen\": 16,\n",
    "    \"seventeen\": 17,\n",
    "    \"eighteen\": 18,\n",
    "    \"nineteen\": 19,\n",
    "    \"twenty\": 20,\n",
    "    \"thirty\": 30,\n",
    "    \"forty\": 40,\n",
    "    \"fifty\": 50,\n",
    "    \"sixty\": 60,\n",
    "    \"seventy\": 70,\n",
    "    \"eighty\": 80,\n",
    "    \"ninety\": 90,\n",
    "    \"hundred\": 100,\n",
    "    \"couple\": 2}\n",
    "\n",
    "string_map = {\n",
    "    \"year\": \"year\",\n",
    "    \"yrs\": \"year\",\n",
    "    \"yr\": \"year\",\n",
    "    \"y\": \"year\",\n",
    "    \"day\": \"day\",\n",
    "    \"week\": \"week\",\n",
    "    \"wk\": \"week\",\n",
    "    \"month\": \"month\"}\n",
    "\n",
    "seasons = ['winter', 'spring', 'summer', 'fall']\n",
    "seasons_map = {\n",
    "    \"winter\": \"january\",\n",
    "    \"spring\": \"april\",\n",
    "    \"summer\": \"july\",\n",
    "    \"fall\": \"october\"}\n",
    "obscure = ['few', 'several', 'many', 'some', 'number', 'plenty', 'various']\n",
    "\n",
    "# Matching: 'a year', 'a month', 'two years ago', '3 years', '2.5 years'\n",
    "regex_1 = \"((\\d+(\\.\\d+)?|(\" + numbers + \"[-\\s]?))\\+?\\s*\" + dwmy + \"s?\\s*(?:\" + suffix + \")?)\"\n",
    "reg_1 = re.compile(regex_1, re.IGNORECASE|re.VERBOSE)\n",
    "\n",
    "# Matching: '2011', 'August 2011', 'Since 2012'\n",
    "regex_2 = \"((?<=\\s)\\d{4}|^\\d{4})\"\n",
    "reg_2 = re.compile(regex_2)\n",
    "\n",
    "# Matching: 11/21/2015 or 11-21-2015\n",
    "regex_3 = \".*?(\\d+[/-]\\d+[/-]\\d+).*?\"\n",
    "reg_3 = re.compile(regex_3, re.IGNORECASE)\n",
    "\n",
    "# Matching: Date in ISO format\n",
    "regex_4 = \"\\d+[/-]\\d+[/-]\\d+ \\d+:\\d+:\\d+\\.\\d+\"\n",
    "reg_4 = re.compile(regex_4)\n",
    "\n",
    "# Matching: 'Several years', 'few months', 'almost 3 years'\n",
    "regex_5 = \"((?:\" + prefix + ')?\\s*(?:(\\d+))?\\s*' +  '(?:' + dwmy + \"s?\" + ')?)' \n",
    "reg_5 = re.compile(regex_5, re.IGNORECASE)\n",
    "\n",
    "# Matching: 'two and a half years'\n",
    "regex_6 = \"((\\d+|\" + numbers + ')\\s*(?=and a).*?(?<=half )' + dwmy + 's?)'\n",
    "reg_6 = re.compile(regex_6, re.IGNORECASE)\n",
    "\n",
    "# Matching: 'The August of 2014' or 'Dec 2006'\n",
    "regex_7 = '.?(' + month + '|' + month_short + ').*?(\\d{4})'\n",
    "reg_7 = re.compile(regex_7, re.IGNORECASE)\n",
    "\n",
    "# Matching: 'Around 2014', 'Since 2011'\n",
    "regex_8 = \".*?((?:\" + prefix + ')?\\s*(?:(\\d+))?\\s*' +  '(?:' + dwmy + \"s?\" + ')?)' \n",
    "\n",
    "# Matching: 'last fall', 'last march'\n",
    "regex_9 = \"((\" + prefix + ')\\s*(' + suffix + '|' + dwmy + '|' + month + '|' + month_short +')\\s*)'\n",
    "reg_9 = re.compile(regex_9, re.IGNORECASE)\n",
    "\n",
    "# Matching: 'couple of months ago', 'couple years ago', 'couple years'\n",
    "regex_10 = \"((\" + prefix + ').*?\\s*(' + dwmy + ').*?\\s*(?:' + suffix + ')?.*?\\s*)'\n",
    "reg_10 = re.compile(regex_10, re.IGNORECASE)\n",
    "\n",
    "# Matching: '\n",
    "regex_11 = '.*?(?:(a))?\\s*?(months|month|days|day|weeks|week|years|year)s?\\s*?(?:(ago))?'\n",
    "reg_11 = re.compile(regex_11, re.IGNORECASE)\n",
    "\n",
    "def text_parse(text):\n",
    "    text = text.lower()\n",
    "    # Match variations of: a year, two years ago, 3 years, 2.5 years, 2+ years\n",
    "    found = reg_1.findall(text)\n",
    "    #print found\n",
    "    if len(found) > 0:\n",
    "        a = found[0][1]\n",
    "        d = found[0][5]\n",
    "        if a.strip() == 'a':\n",
    "            b = a\n",
    "            c = string_map[d]\n",
    "            temp = ' '.join([b, c])\n",
    "            date = dateparser.parse(temp)\n",
    "        elif (a.strip()).isalpha():\n",
    "            b = str(number_map[a.strip()])\n",
    "            c = string_map[d]\n",
    "            temp = ' '.join([b, c])\n",
    "            date = dateparser.parse(temp)\n",
    "        else:\n",
    "            b = str(np.int(np.floor(np.float(a))))\n",
    "            c = string_map[d]\n",
    "            temp = ' '.join([b, c])\n",
    "            date = dateparser.parse(temp)\n",
    "        return date\n",
    "    \n",
    "    # Extract YYYY from variations of '2011', 'August 2011', 'Since 2012'\n",
    "    found2 = reg_2.findall(text)\n",
    "    print found2\n",
    "    #if len(found2) > 0:\n",
    "        #return dateparser.parse(found2[0])\n",
    "    \n",
    "    # Match variations of: 'date is 6/21/16 right?', i.e. 'string MM/DD/YY or MM/DD/YYYY string'\n",
    "    found3 = reg_3.findall(text)\n",
    "    #print found3\n",
    "    if len(found3) > 0:\n",
    "        temp = found3[0]\n",
    "        date = dateparser.parse(temp)\n",
    "        return date\n",
    "    \n",
    "    # Matching: Date in ISO format - No need as dateparser parses it\n",
    "    found4 = reg_4.findall(text)\n",
    "    #print found4\n",
    "    \n",
    "    # Match variations of: 'two and a half years', '3 and a half years'\n",
    "    found6 = reg_6.findall(text)\n",
    "    #print found6\n",
    "    if len(found6) > 0:\n",
    "        a = found6[0][1]\n",
    "        d = found6[0][3]\n",
    "        if (a.strip()).isalpha():\n",
    "            b = str(number_map[a.strip()])\n",
    "            c = string_map[d]\n",
    "            temp = ' '.join([b, c])\n",
    "            date = dateparser.parse(temp)\n",
    "        else:\n",
    "            b = str(np.int(np.floor(np.float(a))))\n",
    "            c = string_map[d]\n",
    "            temp = ' '.join([b, c])\n",
    "            date = dateparser.parse(temp)\n",
    "        return date\n",
    "    \n",
    "    # Match variations of: 'In sept. 2008', 'Last June in 2008' \n",
    "    found7 = reg_7.findall(text)\n",
    "    #print 'found7---', found7\n",
    "    if len(found7) > 0:\n",
    "        a = found7[0][0]\n",
    "        b = found7[0][3]\n",
    "        temp = ' '.join([a, str(b)])\n",
    "        date = dateparser.parse(temp)\n",
    "        return date\n",
    "    \n",
    "    # Match variations of: 'last august', 'last fall'\n",
    "    found9 = reg_9.findall(text)\n",
    "    #print found9\n",
    "    if len(found9) > 0:\n",
    "        crnt_date = datetime.date.today()\n",
    "        a = found9[0][1]\n",
    "        b = found9[0][3]\n",
    "        if a.strip() == 'last':\n",
    "            last_yr = str(crnt_date.year - 1)\n",
    "            if b in seasons:\n",
    "                e = seasons_map[b]\n",
    "                temp = ' '.join([e, last_yr])\n",
    "                date = dateparser.parse(temp)\n",
    "            else:\n",
    "                e = b\n",
    "                temp = ' '.join([e, last_yr])\n",
    "                date = dateparser.parse(temp)\n",
    "            return date\n",
    "        \n",
    "    # Match variations of: 'couple years', 'couple of months'\n",
    "    found10 = reg_10.findall(text)\n",
    "    #print found10\n",
    "    if len(found10) > 0:\n",
    "        a = found10[0][1]\n",
    "        b = found10[0][3]  \n",
    "        if a.strip() == 'couple':\n",
    "            f = str(number_map[a.strip()])\n",
    "            temp = ' '.join([f, b])\n",
    "            date = dateparser.parse(temp)\n",
    "        elif a.strip() in obscure:\n",
    "            f = str(np.random.randint(3, 10))\n",
    "            temp = ' '.join([f, b])\n",
    "            date = dateparser.parse(temp)\n",
    "        else:\n",
    "            date = ''\n",
    "        return date\n",
    "                \n",
    "#print text_parse('3-4 years ago')\n",
    "#print text_parse('2 years and six months')\n",
    "#print reg_10.findall('a couple of months ago')\n",
    "#print reg_10.findall('few months ago')\n",
    "#print reg_10.findall('many years ago')\n",
    "#print text_parse('a couple of months ago')\n",
    "#print text_parse('few months ago')\n",
    "#print text_parse('many years ago')\n",
    "#print text_parse('several weeks ago')\n",
    "#print text_parse('number of weeks ago')\n",
    "#print text_parse('various weeks')\n",
    "print text_parse('since 2012')\n",
    "print text_parse('i think a year ago sure')\n",
    "print text_parse('years ago')\n",
    "print text_parse('month ago')\n",
    "print text_parse('half a year')\n",
    "print text_parse('years ago')\n",
    "print text_parse('year and a half')\n",
    "\n",
    "\n",
    "#under a year\n",
    "#year and a half\n",
    "#Less than a month\n",
    "#For a year and a half.\n",
    "#years ago\n",
    "#I don't remember.   years ago\n",
    "#I year ago\n",
    "#over a year\n",
    "#month ago\n",
    "#over a year ago\n",
    "#At least a year ago.\n",
    "#Half a year\n",
    "#over a year ago\n",
    "#month ago\n",
    "#Over a year ago\n",
    "#Years ago.\n",
    "#A little over a year ago.\n",
    "#I don't know about a year ago, a little less.\n",
    "#Over a year ago.\n",
    "#More than a year ago\n",
    "#Almost a year ago.\n",
    "#year ago\n",
    "#month ago\n",
    "#More than a year ago\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is yield statement in Python?\n",
    "- [Reference](http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "[0, 1, 4]\n",
      "<generator object <genexpr> at 0x1094443c0>\n",
      "0\n",
      "1\n",
      "4\n",
      "<generator object create_generator at 0x109444370>\n",
      "i =  0\n",
      "1\n",
      "i =  1\n",
      "1\n",
      "i =  2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "## Reading a lists item one by one: Iteration\n",
    "# Create list (an iterable)\n",
    "temp_list = [1, 2, 3]\n",
    "for i in temp_list:\n",
    "    print i\n",
    "\n",
    "## List Comprehension creates a list (an iterable)\n",
    "temp_list = [x*x for x in range(3)]\n",
    "print temp_list\n",
    "\n",
    "## Generators: Generators are iterators (can only iterate over them once)\n",
    "# because generators do not store all values in memory, they generate\n",
    "# values on the fly\n",
    "temp_generator = (x*x for x in range(3)) # Used () instead of []\n",
    "print temp_generator\n",
    "\n",
    "# Now iterate over generator\n",
    "for i in temp_generator:\n",
    "    print i\n",
    "    \n",
    "## \"yield\" is a keyword that is used like \"return\", except the function will return a generator\n",
    "def create_generator():\n",
    "    temp_list = range(3)\n",
    "    for i in temp_list:\n",
    "        print 'i = ', i\n",
    "        yield i**i\n",
    "        \n",
    "# Note: When the function is called, the code in the function body does not run   \n",
    "temp_generator = create_generator() # create_generator() is called\n",
    "# Calling function only returns the generator object\n",
    "print temp_generator # print i in create_generator() did not run\n",
    "\n",
    "# Now iterate over generator\n",
    "for i in temp_generator:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
