{
 "metadata": {
  "name": "",
  "signature": "sha256:3b9d84443a67d26191ca01d61582913879c0ad5f6e95a49ee9fe1ce62b71c1b6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Extracting Information from Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import re\n",
      "import pprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Information Extraction\n",
      "\n",
      "- Convert the **unstructured data** of natural language sentences into the structured data, then using query tools like SQL to extract meaning from text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Structured Location data: (entity, relation, entity)\n",
      "locs = [('Google', 'IN', 'Mountain View'), \n",
      "        ('Facebook', 'IN', 'Melno Park'),\n",
      "        ('Amazon', 'IN', 'Seattle'),\n",
      "        ('NYTimes', 'IN', 'New York'),\n",
      "        ('JPL', 'IN', 'Pasadena')]\n",
      "query = [e1 for (e1, rel, e2) in locs if e2 == 'Mountain View']\n",
      "print query"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Google']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**<font color = 'blue'>Information Extraction Architecture</font>**\n",
      "\n",
      "1. Raw text of document is split into sentences using a **Sentence Segmenter**\n",
      "2. Each sentence is further subdivided into words using a **Tokenizer**\n",
      "3. Each sentence is tagged with **Part-Of-Speech** tags\n",
      "4. **Entity Detection** - Search for mentions of potentially interesting entities in each sentence\n",
      "5. **Relation Detection** - Search for likely relations between different entities in the text\n",
      "\n",
      "**Function to perform first 3 tasks:**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ie_preprocess(document):\n",
      "    # Sentence Segmenter\n",
      "    sentences = nltk.sent_tokenize(document)\n",
      "    # Word Tokenizer\n",
      "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "    # POS Tagging\n",
      "    sentences = [nltk.pos_tag(sent) for sent in sentences]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Entity Detection - Segment and label the entities that might participate in interesting relations with one another\n",
      "- Relation Extraction - Search for specific patterns between pairs of entities that occur near one another in the text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Chunking  \n",
      "\n",
      "- **Chunking** - Technique used for **Entity Detection**\n",
      "- Regular Expression and N-Gram approaches\n",
      "- Noun Phrase Chunking - Search for chunks (multi-token sequences) corresponding to individual noun phrases: **<font color = 'blue'>nltk.RegexpParser(Tag Patterns)</font>**, grammar (Tag Patterns) needs to be defined, check help\n",
      "- **<font color = 'blue'>.parse(tokenized and tagged sentence)</font>** and **<font color = 'blue'>.draw( )</font>** methods\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'),\n",
      "            ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('cat', 'NN')]\n",
      "\n",
      "grammar = 'NP: {<DT>?<JJ>*<NN>}'\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "result = cp.parse(sentence)\n",
      "print result\n",
      "result.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
        "  barked/VBD\n",
        "  at/IN\n",
        "  (NP cat/NN))\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tag Patterns\n",
      "\n",
      "- Chunk grammar use **Tag Patterns** to describe sequences of tagged words. \n",
      "- Tag Pattern is a sequence of part of speech tags delimited using angle brackets "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Chunking with Regular Expressions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [('Rapunzel', 'NNP'), ('let', 'VBD'), ('down', 'RP'),\n",
      "            ('her', 'PP$'), ('long', 'JJ'), ('golden', 'JJ'), ('hair', 'NN')]\n",
      "\n",
      "grammar = r'''\n",
      "NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and a noun\n",
      "    {<NNP>+} # chunk sequences of proper noun\n",
      "    '''\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print cp.parse(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP Rapunzel/NNP)\n",
        "  let/VBD\n",
        "  down/RP\n",
        "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nouns = [('money', 'NN'), ('market', 'NN'), ('fund', 'NN')]\n",
      "grammar = 'NP: {<NN><NN>}' # chunk two consecutive nouns\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print cp.parse(nouns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S (NP money/NN market/NN) fund/NN)\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exploring Text Corpora"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser('CHUNK: {<V.*><TO><V.*>}')\n",
      "brown = nltk.corpus.brown\n",
      "for sent in brown.tagged_sents()[:50]: # First 50 sentences\n",
      "    tree = cp.parse(sent)\n",
      "    for subtree in tree.subtrees():\n",
      "        if subtree.label() == 'CHUNK': print subtree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(CHUNK combined/VBN to/TO achieve/VB)\n",
        "(CHUNK continue/VB to/TO place/VB)\n",
        "(CHUNK serve/VB to/TO protect/VB)\n",
        "(CHUNK wanted/VBD to/TO wait/VB)\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Chinking** - The process of removing a sequence of tokens from a chunk. Check Chiking rules in the book"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
      "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), \n",
      "            (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      "grammar = r'''\n",
      "        NP:\n",
      "        {<.*>+} # Chunk everything\n",
      "        }<VBD|IN>+{ # Chink sequences of VBD and IN from NP\n",
      "'''\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print cp.parse(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
        "  barked/VBD\n",
        "  at/IN\n",
        "  (NP the/DT cat/NN))\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Representing Chunks: Tags vs Trees**\n",
      "\n",
      "- **IOB Tags** - Each token is tagged with one of the three special chunk tags - I: Inside or O: Outside or B: Begin"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Developing and Evaluating Chunkers\n",
      "\n",
      "**Reading IOB format**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = '''\n",
      "he PRP B-NP\n",
      "accepted VBD B-VP\n",
      "the DT B-NP\n",
      "position NN I-NP\n",
      "of IN B-PP\n",
      "vice NN B-NP\n",
      "chairman NN I-NP\n",
      "of IN B-PP\n",
      "Carlyle NNP B-NP\n",
      "Group NNP I-NP\n",
      ", , O\n",
      "a DT B-NP\n",
      "merchant NN I-NP\n",
      "banking NN I-NP\n",
      "concern NN I-NP\n",
      ". . O\n",
      "'''\n",
      "nltk.chunk.conllstr2tree(text, chunk_types = ['NP']).draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**CoNLL 2000** Corpus - annotated with POS tags and Chunk tags in IOB format (test and train sets too)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import conll2000\n",
      "print conll2000.chunked_sents('train.txt')[99]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (PP Over/IN)\n",
        "  (NP a/DT cup/NN)\n",
        "  (PP of/IN)\n",
        "  (NP coffee/NN)\n",
        "  ,/,\n",
        "  (NP Mr./NNP Stone/NNP)\n",
        "  (VP told/VBD)\n",
        "  (NP his/PRP$ story/NN)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print conll2000.chunked_sents('train.txt', chunk_types = ['NP'])[99]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  Over/IN\n",
        "  (NP a/DT cup/NN)\n",
        "  of/IN\n",
        "  (NP coffee/NN)\n",
        "  ,/,\n",
        "  (NP Mr./NNP Stone/NNP)\n",
        "  told/VBD\n",
        "  (NP his/PRP$ story/NN)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import conll2000\n",
      "cp = nltk.RegexpParser(\"\")\n",
      "test_sents = conll2000.chunked_sents('test.txt', chunk_types = ['NP'])\n",
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  43.4%\n",
        "    Precision:      0.0%\n",
        "    Recall:         0.0%\n",
        "    F-Measure:      0.0%\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r'NP: {<[CDJNP].*>+}'\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  87.7%\n",
        "    Precision:     70.6%\n",
        "    Recall:        67.8%\n",
        "    F-Measure:     69.2%\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class UnigramChunker(nltk.ChunkParserI):\n",
      "    def __init__(self, train_sents):\n",
      "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
      "        self.tagger = nltk.UnigramTagger(train_data)\n",
      "        \n",
      "        def parse(self, sentence):\n",
      "            pos_tags = [pos for (word, pos) in sentence]\n",
      "            tagged_pos_tags = self.tagger.tag(pos_tags)\n",
      "            chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
      "            conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
      "            return nltk.chunk.conlltags2tree(conlltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Nested Structure with Cascaded Chunkers** - Multi-stage chunk grammar"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
      "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
      "\n",
      "grammar = r'''\n",
      "        NP: {<DT|JJ|NN.*>+} # Chunk sequences of DT, JJ, NN\n",
      "        PP: {<IN><NP>} # Chunk prepositions followed by NP\n",
      "        VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
      "        CLAUSE: {<NP><VP>}\n",
      "'''\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print cp.parse(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP Mary/NN)\n",
        "  saw/VBD\n",
        "  (CLAUSE\n",
        "    (NP the/DT cat/NN)\n",
        "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
      "     (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
      "     (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
      "\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print cp.parse(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP John/NNP)\n",
        "  thinks/VBZ\n",
        "  (NP Mary/NN)\n",
        "  saw/VBD\n",
        "  (CLAUSE\n",
        "    (NP the/DT cat/NN)\n",
        "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color = 'red'>Fails to identify verb phrase headed by \"saw\"</font>, solution: add argument **loop** to specify the number of times the set of patterns should be run"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cp = nltk.RegexpParser(grammar, loop = 2)\n",
      "print cp.parse(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP John/NNP)\n",
        "  thinks/VBZ\n",
        "  (CLAUSE\n",
        "    (NP Mary/NN)\n",
        "    (VP\n",
        "      saw/VBD\n",
        "      (CLAUSE\n",
        "        (NP the/DT cat/NN)\n",
        "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Trees\n",
      "\n",
      "- Tree - A set of connected label nodes, each reachable by a unique path from a distinguished root node. ```s``` is the **parent** of ```VP``` or ```VP``` is the **child** of ```s```, ```VP``` and ```NP``` are both children of ```s```, they are also **siblings**\n",
      "- In NLTK create a tree by giving a node label and a list of children: **<font color = 'blue'>nltk.Tree(node = label, leaves = children)</font>**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree_1 = nltk.Tree('NP', ['Alice'])\n",
      "print tree_1\n",
      "tree_2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
      "print tree_2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(NP Alice)\n",
        "(NP the rabbit)\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Incorporating smaller trees into successively larger trees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree_3 = nltk.Tree('VP', ['chased', tree_2])\n",
      "tree_4 = nltk.Tree('S', [tree_1, tree_3])\n",
      "print tree_4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S (NP Alice) (VP chased (NP the rabbit)))\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **.label( )** - Get node label\n",
      "- **.leaves( )** - Get leaves\n",
      "- **.draw( )** - Draw tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tree_4[1]\n",
      "print tree_4[1].label()\n",
      "print tree_4.leaves()\n",
      "print tree_4[1][1][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(VP chased (NP the rabbit))\n",
        "VP\n",
        "['Alice', 'chased', 'the', 'rabbit']\n",
        "rabbit\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree_3.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Named Entity Recognition (NER)\n",
      "\n",
      "- Named Entities - Definite noun phrases that refer to specific types of individuals such as organizations, persons, dates, etc.\n",
      "- Goal: Identify all textual mentions of the named entities\n",
      "    - Identify the boundaries of named entities\n",
      "    - Identity the named entity's type\n",
      "- <font color='blue'>**nltk.ne_chunk(sentence)**</font> - Classifier to recognize named entities"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
      "print nltk.ne_chunk(sent, binary = True)\n",
      "print nltk.ne_chunk(sent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  The/DT\n",
        "  (NE U.S./NNP)\n",
        "  is/VBZ\n",
        "  one/CD\n",
        "  of/IN\n",
        "  the/DT\n",
        "  few/JJ\n",
        "  industrialized/VBN\n",
        "  nations/NNS\n",
        "  that/WDT\n",
        "  *T*-7/-NONE-\n",
        "  does/VBZ\n",
        "  n't/RB\n",
        "  have/VB\n",
        "  a/DT\n",
        "  higher/JJR\n",
        "  standard/NN\n",
        "  of/IN\n",
        "  regulation/NN\n",
        "  for/IN\n",
        "  the/DT\n",
        "  smooth/JJ\n",
        "  ,/,\n",
        "  needle-like/JJ\n",
        "  fibers/NNS\n",
        "  such/JJ\n",
        "  as/IN\n",
        "  crocidolite/NN\n",
        "  that/WDT\n",
        "  *T*-1/-NONE-\n",
        "  are/VBP\n",
        "  classified/VBN\n",
        "  *-5/-NONE-\n",
        "  as/IN\n",
        "  amphobiles/NNS\n",
        "  ,/,\n",
        "  according/VBG\n",
        "  to/TO\n",
        "  (NE Brooke/NNP)\n",
        "  T./NNP\n",
        "  Mossman/NNP\n",
        "  ,/,\n",
        "  a/DT\n",
        "  professor/NN\n",
        "  of/IN\n",
        "  pathlogy/NN\n",
        "  at/IN\n",
        "  the/DT\n",
        "  (NE University/NNP)\n",
        "  of/IN\n",
        "  (NE Vermont/NNP College/NNP)\n",
        "  of/IN\n",
        "  (NE Medicine/NNP)\n",
        "  ./.)\n",
        "(S\n",
        "  The/DT\n",
        "  (GPE U.S./NNP)\n",
        "  is/VBZ\n",
        "  one/CD\n",
        "  of/IN\n",
        "  the/DT\n",
        "  few/JJ\n",
        "  industrialized/VBN\n",
        "  nations/NNS\n",
        "  that/WDT\n",
        "  *T*-7/-NONE-\n",
        "  does/VBZ\n",
        "  n't/RB\n",
        "  have/VB\n",
        "  a/DT\n",
        "  higher/JJR\n",
        "  standard/NN\n",
        "  of/IN\n",
        "  regulation/NN\n",
        "  for/IN\n",
        "  the/DT\n",
        "  smooth/JJ\n",
        "  ,/,\n",
        "  needle-like/JJ\n",
        "  fibers/NNS\n",
        "  such/JJ\n",
        "  as/IN\n",
        "  crocidolite/NN\n",
        "  that/WDT\n",
        "  *T*-1/-NONE-\n",
        "  are/VBP\n",
        "  classified/VBN\n",
        "  *-5/-NONE-\n",
        "  as/IN\n",
        "  amphobiles/NNS\n",
        "  ,/,\n",
        "  according/VBG\n",
        "  to/TO\n",
        "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
        "  ,/,\n",
        "  a/DT\n",
        "  professor/NN\n",
        "  of/IN\n",
        "  pathlogy/NN\n",
        "  at/IN\n",
        "  the/DT\n",
        "  (ORGANIZATION University/NNP)\n",
        "  of/IN\n",
        "  (PERSON Vermont/NNP College/NNP)\n",
        "  of/IN\n",
        "  (GPE Medicine/NNP)\n",
        "  ./.)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Relation Extraction\n",
      "\n",
      "- Once named entities have been identified in a text, next step is to extract the relations that exist between them\n",
      "- Look for all triples of the form **(X, $\\alpha$, Y)**, where X and Y are named entities, and $\\alpha$ is the string of words that intervenes between X and Y\n",
      "- Using Regular Expression to pull out instances of $\\alpha$ that express the relation\n",
      "- **re.compile(pattern)**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
      "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
      "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus = 'ieer', pattern = IN):\n",
      "        print nltk.sem.rtuple(rel)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ORG: u'WHYY'] u'in' [LOC: u'Philadelphia']\n",
        "[ORG: u'McGlashan &AMP; Sarrail'] u'firm in' [LOC: u'San Mateo']\n",
        "[ORG: u'Freedom Forum'] u'in' [LOC: u'Arlington']\n",
        "[ORG: u'Brookings Institution'] u', the research group in' [LOC: u'Washington']\n",
        "[ORG: u'Idealab'] u', a self-described business incubator based in' [LOC: u'Los Angeles']\n",
        "[ORG: u'Open Text'] u', based in' [LOC: u'Waterloo']\n",
        "[ORG: u'WGBH'] u'in' [LOC: u'Boston']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ORG: u'Bastille Opera'] u'in' [LOC: u'Paris']\n",
        "[ORG: u'Omnicom'] u'in' [LOC: u'New York']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ORG: u'DDB Needham'] u'in' [LOC: u'New York']\n",
        "[ORG: u'Kaplan Thaler Group'] u'in' [LOC: u'New York']\n",
        "[ORG: u'BBDO South'] u'in' [LOC: u'Atlanta']\n",
        "[ORG: u'Georgia-Pacific'] u'in' [LOC: u'Atlanta']\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import conll2002\n",
      "vnv = \"\"\"(\n",
      "is/V|    # 3rd sing present and\n",
      "was/V|   # past forms of the verb zijn ('be')\n",
      "werd/V|  # and also present\n",
      "wordt/V  # past of worden ('become)\n",
      ")\n",
      ".*       # followed by anything\n",
      "van/Prep # followed by van ('of')\n",
      "\"\"\"\n",
      "VAN = re.compile(vnv, re.VERBOSE)\n",
      "for doc in conll2002.chunked_sents('ned.train'):\n",
      "    for r in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='conll2002', pattern=VAN):\n",
      "        print(nltk.sem.clause(r, relsym=\"VAN\"))\n",
      "        print nltk.sem.rtuple(rel, lcon = True, rcon= True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "VAN(u\"cornet_d'elzius\", u'buitenlandse_handel')\n",
        "...u', a spokesman for')[ORG: u'Georgia-Pacific'] u'in' [LOC: u'Atlanta'](u'. Billings were estimated at'...\n",
        "VAN(u'johan_rottiers', u'kardinaal_van_roey_instituut')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "...u', a spokesman for')[ORG: u'Georgia-Pacific'] u'in' [LOC: u'Atlanta'](u'. Billings were estimated at'...\n",
        "VAN(u'annie_lennox', u'eurythmics')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "...u', a spokesman for')[ORG: u'Georgia-Pacific'] u'in' [LOC: u'Atlanta'](u'. Billings were estimated at'...\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> ```ConsecutiveNPChunker``` - Machine Learning Techniques based chunker"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}