{
 "metadata": {
  "name": "",
  "signature": "sha256:dffa44f4ffb2b43664f9aa91313cca9110bee4ff1b8e7375d2b46d405d357fbb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Processing Raw Text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import NLTK\n",
      "import nltk \n",
      "\n",
      "# Import NLTK's word tokenizer\n",
      "from nltk import word_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 120
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Accessing Text from Web and Electronic Books"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import urllib: urlopen\n",
      "from urllib import urlopen\n",
      "url = 'http://www.gutenberg.org/files/2554/2554.txt'\n",
      "response = urlopen(url)\n",
      "\n",
      "# Raw contains details like: whitespace, line breaks, and blank lines\n",
      "raw = response.read() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(raw)\n",
      "print len(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'str'>\n",
        "1176896\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Checking contents in raw\n",
      "raw[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n\\r\\nThis eBook is for the u'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Tokenize** - Breaking up the string into words and punctuation for language processing.  \n",
      "**<font color = 'blue'>nltk.word_tokenize(string)</font>** - Returns a list of words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(raw)\n",
      "print type(tokens)\n",
      "print len(tokens)\n",
      "print tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'list'>\n",
        "254352\n",
        "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Creating an NLTK Text from a list**  \n",
      "**<font color = 'blue'>nltk.Text(list)</font>**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens)\n",
      "print type(text) \n",
      "print '\\n'\n",
      "print text[1024:1062]\n",
      "print '\\n'\n",
      "print text.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'nltk.text.Text'>\n",
        "\n",
        "\n",
        "['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.']\n",
        "\n",
        "\n",
        "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
        "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
        "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
        "great deal; Nikodim Fomitch; young man; Ilya Petrovitch; n't know;\n",
        "Project Gutenberg; Dmitri Prokofitch; Andrey Semyonovitch; Hay Market"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "None\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Methods to find content**:  \n",
      ".find(string)  \n",
      ".rfind(string) - to get right index values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print raw.find('PART I')\n",
      "print raw.find(\"End of Project Gutenberg's Crime\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5338\n",
        "1157746\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = raw[5338: 1157746]\n",
      "print raw.find('PART I')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Accessing text from HTML"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BBC News Story\n",
      "url = 'http://news.bbc.co.uk/2/hi/health/2284783.stm'\n",
      "response = urlopen(url)\n",
      "html = response.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "raw = BeautifulSoup(html).get_text()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = word_tokenize(raw)\n",
      "print tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'BBC', u'NEWS', u'|', u'Health', u'|', u'Blondes', u\"'to\", u'die', u'out', u'in']\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens)\n",
      "text.concordance('Edinburgh')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Displaying 3 of 3 matches:\n",
        "Prof Jonathan Rees , University of Edinburgh But they say too few people now ca\n",
        "f dermatology at the University of Edinburgh said it was unlikely blondes would\n",
        "ert Internet links : University of Edinburgh The BBC is not responsible for the\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###<font color = 'blue'>Reading Local Files</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('document.txt') # Second parameter: 'r' default; 'U' Universal\n",
      "raw = f.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print raw[:100]\n",
      "print '\\n'\n",
      "print type(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "These days it\u2019s not unusual for someone on the way to work to receive a text message from her empl\n",
        "\n",
        "\n",
        "<type 'str'>\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Examine current directory for files**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.listdir('.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "['.ipynb_checkpoints',\n",
        " 'Chapter-1.ipynb',\n",
        " 'Chapter-2.ipynb',\n",
        " 'Chapter-3.ipynb',\n",
        " 'Chapter-4.ipynb',\n",
        " 'document.txt',\n",
        " 'output.txt']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Reading one line at a time**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('document.txt')\n",
      "for line in f:\n",
      "    print line.strip() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "These days it\u2019s not unusual for someone on the way to work to receive a text message from her employer saying she\u2019s not needed right then.\n",
        "\n",
        "Although she\u2019s already found someone to pick up her kid from school and arranged for childcare, the work is no longer available and she won\u2019t be paid for it.\n",
        "\n",
        "Just-in-time scheduling like this is the latest new thing, designed to make retail outlets, restaurants, hotels, and other customer-driven businesses more nimble and keep costs to a minimum.\n",
        "\n",
        "Software can now predict up-to-the-minute staffing needs on the basis of information such as traffic patterns, weather, and sales merely hours or possibly minutes before.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```Extracting Text from PDF, MS Word: Use``` **pypdf** ```library```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Local File Language Processing**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = open('document.txt', 'rU').read()\n",
      "print type(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'str'>\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# When we tokenize a string: we produce a list (of words)\n",
      "tokens = word_tokenize(raw)\n",
      "print type(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'list'>\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = [w.lower() for w in tokens]\n",
      "print type(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'list'>\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist = nltk.FreqDist(words)\n",
      "fdist.most_common(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "[(',', 7), ('to', 5), ('and', 5), ('.', 4), ('the', 4)]"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = len(set(words))\n",
      "length = len(words)\n",
      "print 'Lexical Diversity = %0.3f' % (vocab/float(length))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lexical Diversity = 0.739\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Strings: Text processing  \n",
      "help(str) for getting help"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Using \\ for printing '\n",
      "print 'Monty Python\\'s Flying circus'\n",
      "print\"Monty Python's Flying circus\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Monty Python's Flying circus\n",
        "Monty Python's Flying circus\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# * and + operations apply to string but not - or /\n",
      "print'very ' * 3\n",
      "print 'very' + ' ' + 'good'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "very very very \n",
        "very good\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using **,** in a print statement prints in one line"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = \"Monty Python\"\n",
      "for char in sent:\n",
      "    print char, "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "M o n t y   P y t h o n\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Useful String Methods**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'Monty Python!'\n",
      "print s.find('M')\n",
      "print s.rfind('M')\n",
      "print s.index('M')\n",
      "print s.rindex('M')\n",
      "print s.join('!!')\n",
      "print s.split(' ')\n",
      "print s.splitlines()\n",
      "print s.lower()\n",
      "print s.upper()\n",
      "print s.title()\n",
      "print s.strip()\n",
      "print s.replace('!', '#')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "0\n",
        "0\n",
        "0\n",
        "!Monty Python!!\n",
        "['Monty', 'Python!']\n",
        "['Monty Python!']\n",
        "monty python!\n",
        "MONTY PYTHON!\n",
        "Monty Python!\n",
        "Monty Python!\n",
        "Monty Python#\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Translating text into Unicode: **Decode**, and Translating text out of Unicode: **Encode**  \n",
      "```Python's``` **codecs** ```module provides functions to read encoded data into Unicode strings and to write out Unicode strings in encoded form.```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(path)\n",
      "for line in f:\n",
      "    line = line.strip()\n",
      "    print line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pruska Biblioteka Pa\ufffdstwowa. Jej dawne zbiory znane pod nazw\ufffd\n",
        "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
        "Niemc\ufffdw pod koniec II wojny \ufffdwiatowej na Dolny \ufffdl\ufffdsk, zosta\ufffdy\n",
        "odnalezione po 1945 r. na terytorium Polski. Trafi\ufffdy do Biblioteki\n",
        "Jagiello\ufffdskiej w Krakowie, obejmuj\ufffd ponad 500 tys. zabytkowych\n",
        "archiwali\ufffdw, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### <font color = 'blue'>Regular Expressions for Detecting Word Patterns</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import Regular Expressions library\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**re.search(\"pattern\", \"string\")**\n",
      "\n",
      "- **<font color = 'red'>$</font>** - Matches **end** of a string"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[w for w in wordlist if re.search('ed$', w)][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "[u'abaissed', u'abandoned', u'abased', u'abashed', u'abatised']"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>^</font>** - Caret Matches **start** of a string"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[w for w in wordlist if re.search('^a', w)][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[u'a', u'aa', u'aal', u'aalii', u'aam']"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>.</font>** - Wildcard matches  **any single character** in a string"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[w for w in wordlist if re.search('^....$', w)][-5:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "[u'zoom', u'zoon', u'zuza', u'zyga', u'zyme']"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>?</font>** - Specifies that the **previous character is optional** "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# '^e-?mail$' will match both 'email' or 'e-mail'\n",
      "[w for w in text if re.search('^e-?mail', w)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " - **<font color = 'red'>[abc]</font>** - Matches **one of a set of characters**, *T9* system on mobile phones."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "[u'gold', u'golf', u'hold', u'hole']"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>+</font>** - Matches **1 or more instances of previous character** "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[w for w in wordlist if re.search('^[abc]+$', w)][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "[u'a', u'aa', u'aba', u'abac', u'abaca']"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>*</font>** - Matches **0 or more instances of previous character** "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
      "[w for w in chat_words if re.search('^[abc]*$', w)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[u'', u'a', u'aaaaaaaaaaaaaaaaa', u'b', u'bc', u'c', u'ca', u'caca']"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>$-$</font>** - Defines **range of characters**, e.g. **[0-9]** for specifying 0, 1, 2, 3, 4, 5, 6, 7, 8, 9"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[w for w in wordlist if re.search('^[A-Za-z]+$', w)][-5:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "[u'zymotize', u'zymotoxic', u'zymurgy', u'zythem', u'zythum']"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>[^abc]</font>** - Matches **any character other than a or b or c**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# [^AEIOUaeiou]: matches any character other than a vowel\n",
      "[w for w in chat_words if re.search('^[^AEIOUaeiou]+$', w)][-5:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "[u'yw', u\"yw's\", u'zzzzzzzz', u'~', u'~!']"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>\\</font>** -  Special character following backshash can be literally matched e.g. \"**.**\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
      "print [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)][20:25]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'1.01', u'1.1', u'1.125', u'1.14', u'1.1650']\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>\\{m, n}</font>** - Specifies **number of repeats of previous character**  \n",
      "  - {n} - Exactly n repeats\n",
      "  - {n, } - At least n repeats\n",
      "  - { , n} - No more than n repeats\n",
      "  - {m, n} - At least m and no more than n repeats"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'10-day', u'10-lap', u'10-year', u'100-share', u'12-point']\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>|</font>** - Disjunction matches **one of the specified strings**, where as **<font color = 'red'>( )</font>** - Indicate **scope of the operators**. Usage \"w(i|e|ai)t\" matches \"wit\", \"wet\", \"wait\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print [w for w in wsj if re.search('(ed|ing)$', w)][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'62%-owned', u'Absorbed', u'According', u'Adopting', u'Advanced']\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Extracting Word Pieces <font color = 'blue'>re.findall(r\"pattern\", \"string\")</font>** - Finds and extracts all matches of the given regular expression from a string\n",
      "- **<font color = 'red'>r'...'</font>** - Prefixing the string with r **passes the string directly to the _re_ library**. _r_ is used to indicate that string is raw\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word = 'supercalifragilisticexpialidocious'\n",
      "print re.findall(r'[aeiou]', word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[int(n) for n in re.findall('[0-9]+', '2009-12-31')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "[2009, 12, 31]"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.findall(r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]', 'Universal')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 206,
       "text": [
        "['U', 'n', 'v', 'r', 's', 'l']"
       ]
      }
     ],
     "prompt_number": 206
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>?:</font>** - If we want to use the parentheses ( ) to specify the scope of the disjunction **but not to select the material to be output **?: is added"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# without ?:\n",
      "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'investment')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 154,
       "text": [
        "['ment']"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# with ?:\n",
      "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'investment')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 156,
       "text": [
        "['investment']"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>( )( )</font>** - If we want to **split a word into parts**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'investment')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 158,
       "text": [
        "[('invest', 'ment')]"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **<font color = 'red'>< >< ></font>** - Used for **searching multiple words**. e.g. ```**'<a><man>'** finds all instances of \"a man\" in the text```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(nltk.word_tokenize(\"She is a great artist\"))\n",
      "text.findall(r'<a> (<.*>) <artist>')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "great\n"
       ]
      }
     ],
     "prompt_number": 180
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Resource: http://flex.sourceforge.net/manual/Patterns.html#Patterns"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###**<font color = 'blue'>Normalizing Text</font>** \n",
      "- Converting text to lowercase\n",
      "- Stemming: Stripping off affixes\n",
      "- Lemmatization: Removing affixes and making sure that the resulting form is a known word in a dictionary"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = \"\"\"She was eating mangos with old women\"\"\"\n",
      "tokens = word_tokenize(raw)\n",
      "print tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['She', 'was', 'eating', 'mangos', 'with', 'old', 'women']\n"
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**NLTK Stemmers**\n",
      ">- Lancaster Stemmer: **<font color = 'blue'>nltk.LancasterStemmer( )</font>**\n",
      ">- Porter Stemmer: **<font color = 'blue'>nltk.PorterStemmer( )</font>**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "porter = nltk.PorterStemmer()\n",
      "print [porter.stem(t) for t in tokens][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'She', u'wa', u'eat', u'mango', u'with']\n"
       ]
      }
     ],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lancaster = nltk.LancasterStemmer()\n",
      "[lancaster.stem(t) for t in tokens[:5]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 197,
       "text": [
        "['she', 'was', 'eat', 'mango', 'with']"
       ]
      }
     ],
     "prompt_number": 197
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Lemmatization**\n",
      ">- WordNet Lemmatizer: **<font color = 'blue'>nltk.WordNetLemmatizer( )</font>**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wnl = nltk.WordNetLemmatizer()\n",
      "print [wnl.lemmatize(t) for t in tokens]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['She', u'wa', 'eating', u'mango', 'with', 'old', u'woman']\n"
       ]
      }
     ],
     "prompt_number": 198
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###**<font color = 'blue'>Regular Expressions for Tokenizing Text</font>**  \n",
      "####Simple approaches to tokenization: \n",
      "\n",
      "> - Splitting based on **whitespace ' '**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
      "... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
      "... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 199
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print re.split(r' ', raw)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in']\n"
       ]
      }
     ],
     "prompt_number": 200
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> - Splitting based on **tab('\\t')** or **newline ('\\n')** or **any whitespace ('\\s')**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.split(r'[\\t\\n]+', raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 202,
       "text": [
        "[\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\",\n",
        " \"though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\",\n",
        " \"well without--Maybe it's always pepper that makes people hot-tempered,'...\"]"
       ]
      }
     ],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.split(r'\\s+', raw)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 203,
       "text": [
        "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she']"
       ]
      }
     ],
     "prompt_number": 203
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> -**'\\w'** word characters eqivalent to **[a-zA-Z0-9_]** and its complement **'\\W'** is equivalent to all characters other than letters, digits or underscore"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.split(r'\\W+', raw)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 205,
       "text": [
        "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself']"
       ]
      }
     ],
     "prompt_number": 205
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> - **'\\d'** equivalent to any decimal digit **[0-9]**  \n",
      "> - **'\\D'** equivalent to non digit character **[^0-9]**  \n",
      "> - **'\\S'** equivalent to non-whitespace character  \n",
      "> - ** '?x' ** - Strip out the embedded whitespace and comments"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Flex regular expression tool:   http://flex.sourceforge.net/manual/Patterns.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###**<font color = 'blue'>NLTK's Regular Expressions Tokenizer</font>**  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> - **nltk.regexp_tokenize('string', r'pattern')** is similar to **re.findall(r'pattern', 'string')** but is more efficient"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###**<font color = 'blue'>Segmentation</font>**  \n",
      "> - **Sentence Segmentation <font color = 'blue'>nltk.sent_tokenize('text')</font>** is used to segment the text into sentences before tokenizing the text into words."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###**<font color = 'blue'>Formatting: From lists to strings</font>**  \n",
      "**'str'.join(list)** - String join method "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "silly = ['We', 'called', 'him', 'Tortoise', '.']\n",
      "print ' '.join(silly)\n",
      "print '@'.join(silly)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "We called him Tortoise .\n",
        "We@called@him@Tortoise@.\n"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
      "for word in sorted(fdist):\n",
      "    print word, ':', fdist[word]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cat : 3\n",
        "dog : 4\n",
        "snake : 1\n"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**'{string}'.format( )** - String formatting method"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'string'.format('joker')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 214,
       "text": [
        "'string'"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'{}:-{}'.format('cat', 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 211,
       "text": [
        "'cat:-3'"
       ]
      }
     ],
     "prompt_number": 211
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'{} {} {} {}'.format('I', 'am', 'learning', 'Natural Language Processing')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 212,
       "text": [
        "'I am learning Natural Language Processing'"
       ]
      }
     ],
     "prompt_number": 212
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**{position number}** - Formatting with positional argument"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'from {1} to {0} in {2}'.format('60 mph', '0 mph', '3.4 sec')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 213,
       "text": [
        "'from 0 mph to 60 mph in 3.4 sec'"
       ]
      }
     ],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "template = 'Customer has ordered a {} right now'\n",
      "menu = ['pizza', 'fries']\n",
      "for item in menu:\n",
      "    print template.format(item)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Customer has ordered a pizza right now\n",
        "Customer has ordered a fries right now\n"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Writing results to a file**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_file = open('output.txt', 'w')\n",
      "words = ['Eddy', 'is', 'eating', 'pancakes']\n",
      "for word in sorted(words):\n",
      "    output_file.write(word + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 349
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'colorless'\n",
      "print s[:4] + 'u' + s[-5:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "colourless\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'dishes'\n",
      "s[:-2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 74,
       "text": [
        "'dish'"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'running'\n",
      "s[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "'run'"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'nationality'\n",
      "s[:-5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "'nation'"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'undo'\n",
      "s[:-2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 78,
       "text": [
        "'un'"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'preheat'\n",
      "s[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "'pre'"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3) No it is not possible, we still get \"index error\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = 'Monty'\n",
      "s[-10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'Monty'[10:5:-2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "''"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "5) Reverses the string"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'Monty Python'[::-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "'nohtyP ytnoM'"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "6) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.re_show('[a-zA-Z]+', 'joker')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{Joker}\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.re_show('[A-Z][a-z]*', 'Apple')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{Apple}\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.re_show('p[aeiou]{,2}t', 'poet')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{poet}\n"
       ]
      }
     ],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.re_show('\\d+(\\.\\d+)', '15.43')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{15.43}\n"
       ]
      }
     ],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.re_show('([^aeiou][aeiou][^aeiou])*', 'potsonbot')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{potsonbot}\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.re_show('\\w+|[^\\w\\s]+', 'Jokers')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{Jokers}\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "7)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.re_show('(a|an|the)', 'the')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{the}\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.re_show('[0-9]+', '14')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{14}\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "8)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib import urlopen\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def web_content(url):\n",
      "    html = urlopen(url).read().decode('utf8')\n",
      "    return BeautifulSoup(html).get_text()\n",
      "\n",
      "link = 'http://nltk.org/'\n",
      "web_content(link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "u'\\n\\n\\nNatural Language Toolkit \\u2014 NLTK 3.0 documentation\\n\\n\\n\\n      var DOCUMENTATION_OPTIONS = {\\n        URL_ROOT:    \\'./\\',\\n        VERSION:     \\'3.0\\',\\n        COLLAPSE_INDEX: false,\\n        FILE_SUFFIX: \\'.html\\',\\n        HAS_SOURCE:  true\\n      };\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNLTK 3.0 documentation\\n\\nnext |\\n          modules |\\n          index\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNatural Language Toolkit\\xb6\\nNLTK is a leading platform for building Python programs to work with human language data.\\nIt provides easy-to-use interfaces to over 50 corpora and lexical\\nresources such as WordNet,\\nalong with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\\nand an active discussion forum.\\nThanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics,\\nNLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\\nNLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\\nNLTK has been called \\u201ca wonderful tool for teaching, and working in, computational linguistics using Python,\\u201d\\nand \\u201can amazing library to play with natural language.\\u201d\\nNatural Language Processing with Python provides a practical\\nintroduction to programming for language processing.\\nWritten by the creators of NLTK, it guides the reader through the fundamentals\\nof writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\\nand more.\\nThe book is being updated for Python 3 and NLTK 3.\\n(The original Python 2 version is still available at http://nltk.org/book_1ed.)\\n\\nSome simple things you can do with NLTK\\xb6\\nTokenize and tag some text:\\n>>> import nltk\\n>>> sentence = \"\"\"At eight o\\'clock on Thursday morning\\n... Arthur didn\\'t feel very good.\"\"\"\\n>>> tokens = nltk.word_tokenize(sentence)\\n>>> tokens\\n[\\'At\\', \\'eight\\', \"o\\'clock\", \\'on\\', \\'Thursday\\', \\'morning\\',\\n\\'Arthur\\', \\'did\\', \"n\\'t\", \\'feel\\', \\'very\\', \\'good\\', \\'.\\']\\n>>> tagged = nltk.pos_tag(tokens)\\n>>> tagged[0:6]\\n[(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'), (\\'on\\', \\'IN\\'),\\n(\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\')]\\n\\n\\nIdentify named entities:\\n>>> entities = nltk.chunk.ne_chunk(tagged)\\n>>> entities\\nTree(\\'S\\', [(\\'At\\', \\'IN\\'), (\\'eight\\', \\'CD\\'), (\"o\\'clock\", \\'JJ\\'),\\n           (\\'on\\', \\'IN\\'), (\\'Thursday\\', \\'NNP\\'), (\\'morning\\', \\'NN\\'),\\n       Tree(\\'PERSON\\', [(\\'Arthur\\', \\'NNP\\')]),\\n           (\\'did\\', \\'VBD\\'), (\"n\\'t\", \\'RB\\'), (\\'feel\\', \\'VB\\'),\\n           (\\'very\\', \\'RB\\'), (\\'good\\', \\'JJ\\'), (\\'.\\', \\'.\\')])\\n\\n\\nDisplay a parse tree:\\n>>> from nltk.corpus import treebank\\n>>> t = treebank.parsed_sents(\\'wsj_0001.mrg\\')[0]\\n>>> t.draw()\\n\\n\\n\\nNB. If you publish work that uses NLTK, please cite the NLTK book as\\nfollows:\\n\\nBird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O\\u2019Reilly Media Inc.\\n\\n\\nNext Steps\\xb6\\n\\nsign up for release announcements\\njoin in the discussion\\n\\n\\n\\n\\nContents\\xb6\\n\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\n\\n\\nIndex\\nModule Index\\nSearch Page\\n\\n\\n\\n\\n\\n\\n\\nTable Of Contents\\n\\nNLTK News\\nInstalling NLTK\\nInstalling NLTK Data\\nContribute to NLTK\\nFAQ\\nWiki\\nAPI\\nHOWTO\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n            Enter search terms or a module, class or function name.\\n          \\n\\n\\n\\n\\n\\n\\n\\nnext |\\n          modules |\\n          index\\n\\nShow Source\\n\\n\\n\\n        \\xa9 Copyright 2015, NLTK Project.\\n      Last updated on Mar 13, 2015.\\n      Created using Sphinx 1.2.3.\\n    \\n\\n\\n\\n\\n\\n'"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "9)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('corpus.txt') \n",
      "raw = f.read()\n",
      "words = nltk.word_tokenize(raw)\n",
      "print nltk.Text(words)\n",
      "print '\\n'\n",
      "print words\n",
      "print '\\n'\n",
      "print raw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<Text: Santa Monica is a beachfront city in western...>\n",
        "\n",
        "\n",
        "['Santa', 'Monica', 'is', 'a', 'beachfront', 'city', 'in', 'western', 'Los', 'Angeles', 'County', ',', 'California', ',', 'United', 'States', '.', 'The', 'city', 'is', 'named', 'after', 'the', 'Christian', 'saint', ',', 'Monica', '.', 'Situated', 'on', 'Santa', 'Monica', 'Bay', ',', 'it', 'is', 'bordered', 'on', 'three', 'sides', 'by', 'the', 'city', 'of', 'Los', 'Angeles', '\\xe2\\x80\\x93', 'Pacific', 'Palisades', 'to', 'the', 'north', ',', 'Brentwood', 'on', 'the', 'northeast', ',', 'West', 'Los', 'Angeles', 'and', 'Mar', 'Vista', 'on', 'the', 'east', ',', 'and', 'Venice', 'on', 'the', 'southeast', '.', 'Santa', 'Monica', 'is', 'home', 'to', 'many', 'Hollywood', 'celebrities', 'and', 'executives', 'and', 'is', 'a', 'mixture', 'of', 'affluent', 'single-family', 'neighborhoods', ',', 'renters', ',', 'surfers', ',', 'professionals', ',', 'and', 'students', '.', 'The', 'Census', 'Bureau', '2010', 'population', 'for', 'Santa', 'Monica', 'is', '89,736', '.']\n",
        "\n",
        "\n",
        "Santa Monica is a beachfront city in western Los Angeles County, California, United States. The city is named after the Christian saint, Monica. Situated on Santa Monica Bay, it is bordered on three sides by the city of Los Angeles \u2013 Pacific Palisades to the north, Brentwood on the northeast, West Los Angeles and Mar Vista on the east, and Venice on the southeast. Santa Monica is home to many Hollywood celebrities and executives and is a mixture of affluent single-family neighborhoods, renters, surfers, professionals, and students. The Census Bureau 2010 population for Santa Monica is 89,736.\n"
       ]
      }
     ],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pattern = r'''(?x)\n",
      "            \\w+(-\\w+)\n",
      "            | \\d+\n",
      "            |[A-Za-z]+'''\n",
      "print nltk.regexp_tokenize(raw, pattern)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Santa', 'Monica', 'is', 'a', 'beachfront', 'city', 'in', 'western', 'Los', 'Angeles', 'County', 'California', 'United', 'States', 'The', 'city', 'is', 'named', 'after', 'the', 'Christian', 'saint', 'Monica', 'Situated', 'on', 'Santa', 'Monica', 'Bay', 'it', 'is', 'bordered', 'on', 'three', 'sides', 'by', 'the', 'city', 'of', 'Los', 'Angeles', 'Pacific', 'Palisades', 'to', 'the', 'north', 'Brentwood', 'on', 'the', 'northeast', 'West', 'Los', 'Angeles', 'and', 'Mar', 'Vista', 'on', 'the', 'east', 'and', 'Venice', 'on', 'the', 'southeast', 'Santa', 'Monica', 'is', 'home', 'to', 'many', 'Hollywood', 'celebrities', 'and', 'executives', 'and', 'is', 'a', 'mixture', 'of', 'affluent', 'single-family', 'neighborhoods', 'renters', 'surfers', 'professionals', 'and', 'students', 'The', 'Census', 'Bureau', '2010', 'population', 'for', 'Santa', 'Monica', 'is', '89', '736']\n"
       ]
      }
     ],
     "prompt_number": 242
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "10)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
      "print [(word, len(word)) for word in sent]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
       ]
      }
     ],
     "prompt_number": 246
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "11)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = 'Today is May 30, 2015 and I ate Indian food for dinner'\n",
      "raw.split('a')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 247,
       "text": [
        "['Tod', 'y is M', 'y 30, 2015 ', 'nd I ', 'te Indi', 'n food for dinner']"
       ]
      }
     ],
     "prompt_number": 247
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "12)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string = 'Natural'\n",
      "for s in string:\n",
      "    print s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N\n",
        "a\n",
        "t\n",
        "u\n",
        "r\n",
        "a\n",
        "l\n"
       ]
      }
     ],
     "prompt_number": 249
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "13) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string = 'Natural Language Processing'\n",
      "print string.split()\n",
      "print string.split(' ')\n",
      "print string.split('\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Natural', 'Language', 'Processing']\n",
        "['Natural', 'Language', 'Processing']\n",
        "['Natural Language Processing']\n"
       ]
      }
     ],
     "prompt_number": 255
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "14)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = ['Mango', 'Banana', 'Orange', 'Grapes', 'Apple']\n",
      "words.sort()\n",
      "print words\n",
      "words = ['Mango', 'Banana', 'Orange', 'Grapes', 'Apple']\n",
      "print sorted(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Apple', 'Banana', 'Grapes', 'Mango', 'Orange']\n",
        "['Apple', 'Banana', 'Grapes', 'Mango', 'Orange']\n"
       ]
      }
     ],
     "prompt_number": 260
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "15)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '3' * 2\n",
      "print 3 * 2\n",
      "print int('3') * 2\n",
      "print str(3) * 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "33\n",
        "6\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33\n"
       ]
      }
     ],
     "prompt_number": 263
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}