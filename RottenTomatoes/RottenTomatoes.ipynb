{
 "metadata": {
  "name": "",
  "signature": "sha256:1e700c3f5159e0b8887e8831aaddbaef3ec1b0990f6d9c454b5982d059e587af"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sentiment Analysis on Rotten Tomatoes Movie Reviews"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Data from Kaggle's competition: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RottenTomatoes.ipynb  test.tsv              train.tsv\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import necessary libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reading training and testing data\n",
      "train = pd.read_csv('train.tsv', sep = '\\t')\n",
      "test = pd.read_csv('test.tsv', sep = '\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# View training data\n",
      "train.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PhraseId</th>\n",
        "      <th>SentenceId</th>\n",
        "      <th>Phrase</th>\n",
        "      <th>Sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 1</td>\n",
        "      <td> 1</td>\n",
        "      <td> A series of escapades demonstrating the adage ...</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "      <td> A series of escapades demonstrating the adage ...</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                          A series</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 4</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                                 A</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 5</td>\n",
        "      <td> 1</td>\n",
        "      <td>                                            series</td>\n",
        "      <td> 2</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "   PhraseId  SentenceId                                             Phrase  \\\n",
        "0         1           1  A series of escapades demonstrating the adage ...   \n",
        "1         2           1  A series of escapades demonstrating the adage ...   \n",
        "2         3           1                                           A series   \n",
        "3         4           1                                                  A   \n",
        "4         5           1                                             series   \n",
        "\n",
        "   Sentiment  \n",
        "0          1  \n",
        "1          2  \n",
        "2          2  \n",
        "3          2  \n",
        "4          2  "
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# View testing data\n",
      "test.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PhraseId</th>\n",
        "      <th>SentenceId</th>\n",
        "      <th>Phrase</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> 156061</td>\n",
        "      <td> 8545</td>\n",
        "      <td> An intermittently pleasing but mostly routine ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 156062</td>\n",
        "      <td> 8545</td>\n",
        "      <td> An intermittently pleasing but mostly routine ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 156063</td>\n",
        "      <td> 8545</td>\n",
        "      <td>                                                An</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 156064</td>\n",
        "      <td> 8545</td>\n",
        "      <td> intermittently pleasing but mostly routine effort</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 156065</td>\n",
        "      <td> 8545</td>\n",
        "      <td>        intermittently pleasing but mostly routine</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "   PhraseId  SentenceId                                             Phrase\n",
        "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
        "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
        "2    156063        8545                                                 An\n",
        "3    156064        8545  intermittently pleasing but mostly routine effort\n",
        "4    156065        8545         intermittently pleasing but mostly routine"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# special IPython command to prepare the notebook for matplotlib\n",
      "%matplotlib inline "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Unique sentiment classes\n",
      "train['Sentiment'].unique()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([1, 2, 3, 4, 0])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train['Sentiment'].hist(color = 'blue', alpha = 0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x107cbca10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDCrmSQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x107cbc350>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tye of data\n",
      "type(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "pandas.core.frame.DataFrame"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Summary (Works only for numeric data)\n",
      "train.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>PhraseId</th>\n",
        "      <th>SentenceId</th>\n",
        "      <th>Sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 156060.000000</td>\n",
        "      <td> 156060.000000</td>\n",
        "      <td> 156060.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>  78030.500000</td>\n",
        "      <td>   4079.732744</td>\n",
        "      <td>      2.063578</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>  45050.785842</td>\n",
        "      <td>   2502.764394</td>\n",
        "      <td>      0.893832</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>      1.000000</td>\n",
        "      <td>      1.000000</td>\n",
        "      <td>      0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>  39015.750000</td>\n",
        "      <td>   1861.750000</td>\n",
        "      <td>      2.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>  78030.500000</td>\n",
        "      <td>   4017.000000</td>\n",
        "      <td>      2.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td> 117045.250000</td>\n",
        "      <td>   6244.000000</td>\n",
        "      <td>      3.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td> 156060.000000</td>\n",
        "      <td>   8544.000000</td>\n",
        "      <td>      4.000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "            PhraseId     SentenceId      Sentiment\n",
        "count  156060.000000  156060.000000  156060.000000\n",
        "mean    78030.500000    4079.732744       2.063578\n",
        "std     45050.785842    2502.764394       0.893832\n",
        "min         1.000000       1.000000       0.000000\n",
        "25%     39015.750000    1861.750000       2.000000\n",
        "50%     78030.500000    4017.000000       2.000000\n",
        "75%    117045.250000    6244.000000       3.000000\n",
        "max    156060.000000    8544.000000       4.000000"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Extracting features from the text. i.e. Convert text content into numerical feature vector\n",
      "# Bag of words\n",
      "# Tokenizing text with scikit-learn\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "count_vector = CountVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_train_counts = count_vector.fit_transform(train['Phrase'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Dimension of the vector\n",
      "x_train_counts.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "(156060, 15240)"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get index of some common words/N-grams/consequecitive characters\n",
      "count_vector.vocabulary_.get(u'movie')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "8791"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get feature names\n",
      "count_vector.get_feature_names()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[u'000', u'10', u'100', u'101', u'102']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## From occurrences to frequencies: Term Frequencies\n",
      "# Downscaling: Term Frequency times Inverse Document Frequency(tf-idf)\n",
      "from sklearn.feature_extraction.text import TfidfTransformer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Term Frequencies (tf)\n",
      "\n",
      "# Use fit() method to fit estimator to the data\n",
      "tf_transformer = TfidfTransformer(use_idf = False).fit(x_train_counts)\n",
      "\n",
      "# Use transform() method to transform count-matrix to tf representation\n",
      "x_train_tf = tf_transformer.transform(x_train_counts)\n",
      "x_train_tf.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "(156060, 15240)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Term Frequency times Inverse Document Frequency (tf-idf)\n",
      "\n",
      "tfidf_transformer = TfidfTransformer()\n",
      "\n",
      "# Using fit_transform() method\n",
      "x_train_tfidf = tfidf_transformer.fit_transform(x_train_counts)\n",
      "x_train_tfidf.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "(156060, 15240)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Training a classifier to predict sentiment of a phrase\n",
      "# Naive Bayes Classifier\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "clf = MultinomialNB().fit(x_train_tfidf, train['Sentiment'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Prediction on test\n",
      "x_test_counts = count_vector.transform(test['Phrase'])\n",
      "x_test_tfidf = tfidf_transformer.transform(x_test_counts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted = clf.predict(x_test_tfidf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print predicted[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[3 3 2 3 3]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "with open('Rotten_Sentiment.csv', 'w') as csvfile:\n",
      "    csvfile.write('PhraseId,Sentiment\\n')\n",
      "    for i, j in zip(test['PhraseId'], predicted):\n",
      "        csvfile.write('{},{}\\n'.format(i, j))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(CountVectorizer)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
        "\n",
        "class CountVectorizer(sklearn.base.BaseEstimator, VectorizerMixin)\n",
        " |  Convert a collection of text documents to a matrix of token counts\n",
        " |  \n",
        " |  This implementation produces a sparse representation of the counts using\n",
        " |  scipy.sparse.coo_matrix.\n",
        " |  \n",
        " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
        " |  that does some kind of feature selection then the number of features will\n",
        " |  be equal to the vocabulary size found by analyzing the data.\n",
        " |  \n",
        " |  Parameters\n",
        " |  ----------\n",
        " |  input : string {'filename', 'file', 'content'}\n",
        " |      If 'filename', the sequence passed as an argument to fit is\n",
        " |      expected to be a list of filenames that need reading to fetch\n",
        " |      the raw content to analyze.\n",
        " |  \n",
        " |      If 'file', the sequence items must have a 'read' method (file-like\n",
        " |      object) that is called to fetch the bytes in memory.\n",
        " |  \n",
        " |      Otherwise the input is expected to be the sequence strings or\n",
        " |      bytes items are expected to be analyzed directly.\n",
        " |  \n",
        " |  encoding : string, 'utf-8' by default.\n",
        " |      If bytes or files are given to analyze, this encoding is used to\n",
        " |      decode.\n",
        " |  \n",
        " |  decode_error : {'strict', 'ignore', 'replace'}\n",
        " |      Instruction on what to do if a byte sequence is given to analyze that\n",
        " |      contains characters not of the given `encoding`. By default, it is\n",
        " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
        " |      values are 'ignore' and 'replace'.\n",
        " |  \n",
        " |  strip_accents : {'ascii', 'unicode', None}\n",
        " |      Remove accents during the preprocessing step.\n",
        " |      'ascii' is a fast method that only works on characters that have\n",
        " |      an direct ASCII mapping.\n",
        " |      'unicode' is a slightly slower method that works on any characters.\n",
        " |      None (default) does nothing.\n",
        " |  \n",
        " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
        " |      Whether the feature should be made of word or character n-grams.\n",
        " |      Option 'char_wb' creates character n-grams only from text inside\n",
        " |      word boundaries.\n",
        " |  \n",
        " |      If a callable is passed it is used to extract the sequence of features\n",
        " |      out of the raw, unprocessed input.\n",
        " |  \n",
        " |  preprocessor : callable or None (default)\n",
        " |      Override the preprocessing (string transformation) stage while\n",
        " |      preserving the tokenizing and n-grams generation steps.\n",
        " |  \n",
        " |  tokenizer : callable or None (default)\n",
        " |      Override the string tokenization step while preserving the\n",
        " |      preprocessing and n-grams generation steps.\n",
        " |  \n",
        " |  ngram_range : tuple (min_n, max_n)\n",
        " |      The lower and upper boundary of the range of n-values for different\n",
        " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
        " |      will be used.\n",
        " |  \n",
        " |  stop_words : string {'english'}, list, or None (default)\n",
        " |      If 'english', a built-in stop word list for English is used.\n",
        " |  \n",
        " |      If a list, that list is assumed to contain stop words, all of which\n",
        " |      will be removed from the resulting tokens.\n",
        " |  \n",
        " |      If None, no stop words will be used. max_df can be set to a value\n",
        " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
        " |      words based on intra corpus document frequency of terms.\n",
        " |  \n",
        " |  lowercase : boolean, True by default\n",
        " |      Convert all characters to lowercase before tokenizing.\n",
        " |  \n",
        " |  token_pattern : string\n",
        " |      Regular expression denoting what constitutes a \"token\", only used\n",
        " |      if `tokenize == 'word'`. The default regexp select tokens of 2\n",
        " |      or more alphanumeric characters (punctuation is completely ignored\n",
        " |      and always treated as a token separator).\n",
        " |  \n",
        " |  max_df : float in range [0.0, 1.0] or int, optional, 1.0 by default\n",
        " |      When building the vocabulary ignore terms that have a document\n",
        " |      frequency strictly higher than the given threshold (corpus-specific\n",
        " |      stop words).\n",
        " |      If float, the parameter represents a proportion of documents, integer\n",
        " |      absolute counts.\n",
        " |      This parameter is ignored if vocabulary is not None.\n",
        " |  \n",
        " |  min_df : float in range [0.0, 1.0] or int, optional, 1 by default\n",
        " |      When building the vocabulary ignore terms that have a document\n",
        " |      frequency strictly lower than the given threshold. This value is also\n",
        " |      called cut-off in the literature.\n",
        " |      If float, the parameter represents a proportion of documents, integer\n",
        " |      absolute counts.\n",
        " |      This parameter is ignored if vocabulary is not None.\n",
        " |  \n",
        " |  max_features : optional, None by default\n",
        " |      If not None, build a vocabulary that only consider the top\n",
        " |      max_features ordered by term frequency across the corpus.\n",
        " |  \n",
        " |      This parameter is ignored if vocabulary is not None.\n",
        " |  \n",
        " |  vocabulary : Mapping or iterable, optional\n",
        " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
        " |      indices in the feature matrix, or an iterable over terms. If not\n",
        " |      given, a vocabulary is determined from the input documents. Indices\n",
        " |      in the mapping should not be repeated and should not have any gap\n",
        " |      between 0 and the largest index.\n",
        " |  \n",
        " |  binary : boolean, False by default.\n",
        " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
        " |      probabilistic models that model binary events rather than integer\n",
        " |      counts.\n",
        " |  \n",
        " |  dtype : type, optional\n",
        " |      Type of the matrix returned by fit_transform() or transform().\n",
        " |  \n",
        " |  Attributes\n",
        " |  ----------\n",
        " |  `vocabulary_` : dict\n",
        " |      A mapping of terms to feature indices.\n",
        " |  \n",
        " |  `stop_words_` : set\n",
        " |      Terms that were ignored because\n",
        " |      they occurred in either too many\n",
        " |      (`max_df`) or in too few (`min_df`) documents.\n",
        " |      This is only available if no vocabulary was given.\n",
        " |  \n",
        " |  See also\n",
        " |  --------\n",
        " |  HashingVectorizer, TfidfVectorizer\n",
        " |  \n",
        " |  Method resolution order:\n",
        " |      CountVectorizer\n",
        " |      sklearn.base.BaseEstimator\n",
        " |      VectorizerMixin\n",
        " |      __builtin__.object\n",
        " |  \n",
        " |  Methods defined here:\n",
        " |  \n",
        " |  __init__(self, input=u'content', encoding=u'utf-8', charset=None, decode_error=u'strict', charset_error=None, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer=u'word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<type 'numpy.int64'>)\n",
        " |  \n",
        " |  fit(self, raw_documents, y=None)\n",
        " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
        " |      \n",
        " |      Parameters\n",
        " |      ----------\n",
        " |      raw_documents : iterable\n",
        " |          An iterable which yields either str, unicode or file objects.\n",
        " |      \n",
        " |      Returns\n",
        " |      -------\n",
        " |      self\n",
        " |  \n",
        " |  fit_transform(self, raw_documents, y=None)\n",
        " |      Learn the vocabulary dictionary and return term-document matrix.\n",
        " |      \n",
        " |      This is equivalent to fit followed by transform, but more efficiently\n",
        " |      implemented.\n",
        " |      \n",
        " |      Parameters\n",
        " |      ----------\n",
        " |      raw_documents : iterable\n",
        " |          An iterable which yields either str, unicode or file objects.\n",
        " |      \n",
        " |      Returns\n",
        " |      -------\n",
        " |      X : array, [n_samples, n_features]\n",
        " |          Document-term matrix.\n",
        " |  \n",
        " |  get_feature_names(self)\n",
        " |      Array mapping from feature integer indices to feature name\n",
        " |  \n",
        " |  inverse_transform(self, X)\n",
        " |      Return terms per document with nonzero entries in X.\n",
        " |      \n",
        " |      Parameters\n",
        " |      ----------\n",
        " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
        " |      \n",
        " |      Returns\n",
        " |      -------\n",
        " |      X_inv : list of arrays, len = n_samples\n",
        " |          List of arrays of terms.\n",
        " |  \n",
        " |  transform(self, raw_documents)\n",
        " |      Transform documents to document-term matrix.\n",
        " |      \n",
        " |      Extract token counts out of raw text documents using the vocabulary\n",
        " |      fitted with fit or the one provided to the constructor.\n",
        " |      \n",
        " |      Parameters\n",
        " |      ----------\n",
        " |      raw_documents : iterable\n",
        " |          An iterable which yields either str, unicode or file objects.\n",
        " |      \n",
        " |      Returns\n",
        " |      -------\n",
        " |      X : sparse matrix, [n_samples, n_features]\n",
        " |          Document-term matrix.\n",
        " |  \n",
        " |  ----------------------------------------------------------------------\n",
        " |  Methods inherited from sklearn.base.BaseEstimator:\n",
        " |  \n",
        " |  __repr__(self)\n",
        " |  \n",
        " |  get_params(self, deep=True)\n",
        " |      Get parameters for this estimator.\n",
        " |      \n",
        " |      Parameters\n",
        " |      ----------\n",
        " |      deep: boolean, optional\n",
        " |          If True, will return the parameters for this estimator and\n",
        " |          contained subobjects that are estimators.\n",
        " |      \n",
        " |      Returns\n",
        " |      -------\n",
        " |      params : mapping of string to any\n",
        " |          Parameter names mapped to their values.\n",
        " |  \n",
        " |  set_params(self, **params)\n",
        " |      Set the parameters of this estimator.\n",
        " |      \n",
        " |      The method works on simple estimators as well as on nested objects\n",
        " |      (such as pipelines). The former have parameters of the form\n",
        " |      ``<component>__<parameter>`` so that it's possible to update each\n",
        " |      component of a nested object.\n",
        " |      \n",
        " |      Returns\n",
        " |      -------\n",
        " |      self\n",
        " |  \n",
        " |  ----------------------------------------------------------------------\n",
        " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
        " |  \n",
        " |  __dict__\n",
        " |      dictionary for instance variables (if defined)\n",
        " |  \n",
        " |  __weakref__\n",
        " |      list of weak references to the object (if defined)\n",
        " |  \n",
        " |  ----------------------------------------------------------------------\n",
        " |  Methods inherited from VectorizerMixin:\n",
        " |  \n",
        " |  build_analyzer(self)\n",
        " |      Return a callable that handles preprocessing and tokenization\n",
        " |  \n",
        " |  build_preprocessor(self)\n",
        " |      Return a function to preprocess the text before tokenization\n",
        " |  \n",
        " |  build_tokenizer(self)\n",
        " |      Return a function that splits a string into a sequence of tokens\n",
        " |  \n",
        " |  decode(self, doc)\n",
        " |      Decode the input into a string of unicode symbols\n",
        " |      \n",
        " |      The decoding strategy depends on the vectorizer parameters.\n",
        " |  \n",
        " |  get_stop_words(self)\n",
        " |      Build or fetch the effective stop words list\n",
        " |  \n",
        " |  ----------------------------------------------------------------------\n",
        " |  Data descriptors inherited from VectorizerMixin:\n",
        " |  \n",
        " |  fixed_vocabulary\n",
        " |      DEPRECATED: The `fixed_vocabulary` attribute is deprecated and will be removed in 0.18.  Please use `fixed_vocabulary_` instead.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}